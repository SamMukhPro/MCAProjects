{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamMukhPro/MCAProjects/blob/main/Minor_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTQxtqz9wlhK"
      },
      "outputs": [],
      "source": [
        "! pip install emoji\n",
        "!pip install stopwordsiso\n",
        "!pip install nrclex\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install flask-ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNa8K0d6xpt5"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b3Fh3fuwpN4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "import string\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from stopwordsiso import stopwords\n",
        "from nrclex import NRCLex\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# --- Imports ---\n",
        "from transformers import pipeline\n",
        "from googletrans import Translator\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import json, os, csv, random\n",
        "from datetime import datetime\n",
        "import re\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from scipy.sparse import hstack\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKatTOKDxtLh"
      },
      "source": [
        "## File upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hge-Bnx-wsfr"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIoM9iIFxwMN"
      },
      "source": [
        "## Datset read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcCSr-iwwuc4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "df = pd.read_excel(\"dataset5.xlsx\", sheet_name=\"Sheet1\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np5Vke7nx2pT"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXfgovsnxOWS"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "# Bengali Unicode range\n",
        "BN_RANGE = r\"\\u0980-\\u09FF\"\n",
        "\n",
        "# 1.Language detection (fast, regex-based)\n",
        "def detect_lang_fast(text, bn_threshold=0.15):\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return \"unknown\"\n",
        "    bn_chars = len(re.findall(r'[' + BN_RANGE + ']', text))\n",
        "    latin_chars = len(re.findall(r'[A-Za-z]', text))\n",
        "    total_letters = bn_chars + latin_chars\n",
        "\n",
        "    if total_letters == 0:\n",
        "        return \"unknown\"\n",
        "    if bn_chars / (total_letters + 1e-9) >= bn_threshold:\n",
        "        return \"bn\"\n",
        "    elif latin_chars > 0:\n",
        "        return \"en\"\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "# 2.Clean text\n",
        "def clean_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    s = re.sub(fr\"[^a-z{BN_RANGE}\\s\\.\\,\\!\\?\\â€¦']\", \" \", s)  # keep English + Bengali + punctuation\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# 3.Emoji detection\n",
        "def has_emoji(s: str) -> int:\n",
        "\n",
        "    return int(any(char in emoji.EMOJI_DATA for char in str(s)))\n",
        "\n",
        "# 4.Preprocess function\n",
        "def preprocess_text(s: str):\n",
        "    if not isinstance(s, str):\n",
        "        return pd.Series([\"\", \"unknown\", 0, 0],\n",
        "                         index=[\"clean_text\", \"lang\", \"emoji\", \"word_count\"])\n",
        "\n",
        "    cleaned = clean_text(s)\n",
        "    lang = detect_lang_fast(cleaned)\n",
        "    emoji_flag = has_emoji(s)\n",
        "    wc = len(cleaned.split())\n",
        "\n",
        "    return pd.Series([cleaned, lang, emoji_flag, wc],\n",
        "                     index=[\"clean_text\", \"lang\", \"emoji\", \"word_count\"])\n",
        "\n",
        "#  Load dataset\n",
        "df = pd.read_excel(\"dataset5.xlsx\", sheet_name=\"Sheet1\")\n",
        "\n",
        "#  Apply preprocessing to all rows\n",
        "meta_df = df[\"Sentence\"].apply(preprocess_text)\n",
        "cleaned_df = pd.concat([df, meta_df], axis=1)\n",
        "\n",
        "# Filter out empty\n",
        "cleaned_df = cleaned_df[cleaned_df[\"clean_text\"].str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "# Show sample\n",
        "print(cleaned_df.head(5)[[\"Sentence\", \"clean_text\", \"lang\", \"emoji\", \"word_count\", \"Emotion\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBhNY0kvx8BN"
      },
      "source": [
        "## FEATURE ENGINEERING & EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOVLCmENxbRA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Load dataset\n",
        "# =========================\n",
        "# Assuming df is already loaded in a previous cell (e.g., from \"DATASET READ\")\n",
        "# If not, uncomment the line below:\n",
        "# df = pd.read_excel(\"dataset5.xlsx\")\n",
        "texts = df['Sentence'].astype(str) # Corrected column name from 'Text' to 'Sentence'\n",
        "\n",
        "\n",
        "# =========================\n",
        "# STOPWORDS\n",
        "# =========================\n",
        "english_stops = set(stopwords(\"en\"))\n",
        "bengali_stops = set(stopwords(\"bn\"))\n",
        "all_stops = english_stops.union(bengali_stops)\n",
        "\n",
        "# =========================\n",
        "# Expanded Bengali Emotion Lexicon\n",
        "# =========================\n",
        "bengali_emotion_lexicon = {\n",
        "    # Joy / Positive\n",
        "    \"à¦–à§à¦¶à¦¿\": \"joy\", \"à¦†à¦¨à¦¨à§à¦¦\": \"joy\", \"à¦­à¦¾à¦²à§‹\": \"joy\", \"à¦­à¦¾à¦²à§‹à¦¬à¦¾à¦¸à¦¾\": \"joy\",\n",
        "    \"à¦ªà§à¦°à¦¶à¦¾à¦¨à§à¦¤à¦¿\": \"joy\", \"à¦¸à§à¦–\": \"joy\", \"à¦¸à§à¦–à§€\": \"joy\", \"à¦†à¦¶à§€à¦°à§à¦¬à¦¾à¦¦\": \"joy\",\n",
        "    \"à¦¹à¦¾à¦¸à¦¿\": \"joy\", \"à¦‰à§Žà¦¸à¦¬\": \"joy\", \"à¦—à¦°à§à¦¬\": \"joy\", \"à¦šà¦®à§Žà¦•à¦¾à¦°\": \"joy\",\n",
        "    \"à¦ªà§à¦°à¦«à§à¦²à§à¦²\": \"joy\", \"à¦¸à¦¨à§à¦¤à§à¦·à§à¦Ÿ\": \"joy\", \"à¦‰à¦šà§à¦›à§à¦¬à¦¾à¦¸\": \"joy\", \"à¦‰à¦¦à¦¯à¦¾à¦ªà¦¨\": \"joy\",\n",
        "    \"à¦†à¦¶à¦¾\": \"anticipation\", \"à¦¸à§à¦¬à¦ªà§à¦¨\": \"anticipation\", \"à¦…à¦ªà§‡à¦•à§à¦·à¦¾\": \"anticipation\",\n",
        "    \"à¦‰à§Žà¦¸à¦¾à¦¹\": \"anticipation\", \"à¦ªà§à¦°à¦¤à§à¦¯à¦¾à¦¶à¦¾\": \"anticipation\",\n",
        "\n",
        "    # Sadness / Grief\n",
        "    \"à¦¦à§à¦ƒà¦–\": \"sadness\", \"à¦¦à§à¦ƒà¦–à¦¿à¦¤\": \"sadness\", \"à¦•à¦¾à¦¨à§à¦¨à¦¾\": \"sadness\", \"à¦•à¦¾à¦à¦¦à¦›à¦¿\": \"sadness\",\n",
        "    \"à¦…à¦¬à¦¸à¦¾à¦¦\": \"sadness\", \"à¦®à¦¨ à¦–à¦¾à¦°à¦¾à¦ª\": \"sadness\", \"à¦¬à§‡à¦¦à¦¨à¦¾\": \"sadness\",\n",
        "    \"à¦•à¦·à§à¦Ÿ\": \"sadness\", \"à¦¬à¦¿à¦°à¦¹\": \"sadness\", \"à¦¹à¦¾à¦°à¦¾à¦¨à§‹\": \"sadness\", \"à¦à¦•à¦¾à¦•à§€\": \"sadness\",\n",
        "    \"à¦¶à§‹à¦•\": \"sadness\", \"à¦¹à¦¤à¦¾à¦¶à¦¾\": \"sadness\", \"à¦¬à¦¿à¦·à¦£à§à¦£\": \"sadness\",\n",
        "    \"à¦…à¦¸à¦¹à¦¾à¦¯à¦¼\": \"sadness\", \"à¦¬à¦¿à¦ªà¦¦\": \"sadness\", \"à¦…à¦¨à§à¦§à¦•à¦¾à¦°\": \"sadness\",\n",
        "\n",
        "    # Anger / Frustration\n",
        "    \"à¦°à¦¾à¦—\": \"anger\", \"à¦°à¦¾à¦—à¦¾à¦¨à§à¦¬à¦¿à¦¤\": \"anger\", \"à¦¬à¦¿à¦°à¦•à§à¦¤\": \"anger\", \"à¦˜à§ƒà¦£à¦¾\": \"anger\",\n",
        "    \"à¦•à§à¦·à§‹à¦­\": \"anger\", \"à¦†à¦•à§à¦°à§‹à¦¶\": \"anger\", \"à¦à¦—à¦¡à¦¼à¦¾\": \"anger\",\n",
        "    \"à¦…à¦¸à¦¹à§à¦¯\": \"anger\", \"à¦…à¦ªà¦®à¦¾à¦¨\": \"anger\", \"à¦šà¦¿à§Žà¦•à¦¾à¦°\": \"anger\", \"à¦®à¦¾à¦°à¦§à¦°\": \"anger\",\n",
        "    \"à¦¬à¦¿à¦°à§‹à¦§\": \"anger\", \"à¦…à¦­à¦¿à¦¯à§‹à¦—\": \"anger\", \"à¦¹à¦¿à¦‚à¦¸à¦¾\": \"anger\", \"à¦…à¦ªà¦›à¦¨à§à¦¦\": \"anger\",\n",
        "    \"à¦¶à¦¤à§à¦°à§\": \"anger\", \"à¦†à¦•à§à¦°à¦®à¦£\": \"anger\",\n",
        "\n",
        "    # Fear / Anxiety\n",
        "    \"à¦­à¦¯à¦¼\": \"fear\", \"à¦†à¦¤à¦™à§à¦•\": \"fear\", \"à¦šà¦¾à¦ª\": \"fear\", \"à¦‰à¦¦à§à¦¬à¦¿à¦—à§à¦¨\": \"fear\",\n",
        "    \"à¦šà¦¿à¦¨à§à¦¤à¦¾\": \"fear\", \"à¦…à¦¸à§à¦¥à¦¿à¦°\": \"fear\", \"à¦¶à¦™à§à¦•à¦¾\": \"fear\", \"à¦­à¦¯à¦¼à¦‚à¦•à¦°\": \"fear\",\n",
        "    \"à¦†à¦¶à¦™à§à¦•à¦¾\": \"fear\", \"à¦¹à§à¦®à¦•à¦¿\": \"fear\", \"à¦¬à¦¿à¦ªà¦¦\": \"fear\", \"à¦­à§€à¦¤\": \"fear\",\n",
        "    \"à¦¦à§à¦¶à§à¦šà¦¿à¦¨à§à¦¤à¦¾\": \"fear\", \"à¦ªà§ƒà¦¥à¦¿à¦¬à§€à¦° à¦¶à§‡à¦·\": \"fear\", \"à¦à§à¦à¦•à¦¿\": \"fear\", \"à¦šà¦°à¦®\": \"fear\",\n",
        "\n",
        "    # Suicidal / Self-harm\n",
        "    \"à¦†à¦¤à§à¦®à¦¹à¦¤à§à¦¯à¦¾\": \"suicidal\", \"à¦®à§ƒà¦¤à§à¦¯à§\": \"suicidal\", \"à¦®à¦°à¦¤à§‡ à¦šà¦¾à¦‡\": \"suicidal\",\n",
        "    \"à¦¶à§‡à¦· à¦•à¦°à§‡ à¦¦à¦¾à¦“\": \"suicidal\", \"à¦¨à¦¿à¦œà§‡à¦•à§‡ à¦¶à§‡à¦·\": \"suicidal\", \"à¦¨à¦¿à¦œà§‡à¦•à§‡ à¦®à§‡à¦°à§‡ à¦«à§‡à¦²à¦¬\": \"suicidal\",\n",
        "    \"à¦†à¦¤à§à¦®à¦¹à¦¾à¦¨à¦¿\": \"suicidal\", \"à¦—à¦²à¦¾à¦¯à¦¼ à¦¦à¦¡à¦¼à¦¿\": \"suicidal\", \"à¦“à¦·à§à¦§ à¦–à¦¾à¦¬\": \"suicidal\",\n",
        "    \"à¦¬à¦¾à¦à¦šà¦¤à§‡ à¦šà¦¾à¦‡ à¦¨à¦¾\": \"suicidal\", \"à¦¸à¦¬ à¦¶à§‡à¦·\": \"suicidal\", \"à¦œà§€à¦¬à¦¨ à¦¶à§‡à¦·\": \"suicidal\",\n",
        "    \"kill myself\": \"suicidal\", \"end it\": \"suicidal\", \"want to die\": \"suicidal\"\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# Lexical Features\n",
        "# =========================\n",
        "def word_count(text): return len(text.split())\n",
        "def avg_word_length(text):\n",
        "    words = text.split()\n",
        "    return np.mean([len(w) for w in words]) if words else 0\n",
        "def unique_word_count(text): return len(set(text.split()))\n",
        "def character_count(text): return len(text)\n",
        "def stopword_ratio(text):\n",
        "    words = text.split()\n",
        "    return sum(1 for w in words if w.lower() in all_stops) / len(words) if words else 0\n",
        "def punctuation_count(text): return sum(1 for ch in text if ch in string.punctuation)\n",
        "def emoji_count(text): return sum(1 for ch in text if ch in emoji.EMOJI_DATA)\n",
        "def uppercase_ratio(text):\n",
        "    words = text.split()\n",
        "    return sum(1 for w in words if w.isupper()) / len(words) if words else 0\n",
        "\n",
        "# =========================\n",
        "# Syntactic Features\n",
        "# =========================\n",
        "def pos_tag_distribution(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "    return dict(Counter(tag for _, tag in tags))\n",
        "def number_of_sentences(text): return len(nltk.sent_tokenize(text))\n",
        "def sentence_length_variance(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    lengths = [len(s.split()) for s in sentences]\n",
        "    return np.var(lengths) if lengths else 0\n",
        "def use_of_negation(text): return sum(text.count(w) for w in [\"à¦¨à¦¾\",\"à¦¨à§‡à¦‡\",\"à¦ªà¦¾à¦°à¦›à¦¿ à¦¨à¦¾\"])\n",
        "def personal_pronoun_ratio(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "    pronouns = [w for w, t in tags if t in [\"PRP\", \"PRP$\"]]\n",
        "    return len(pronouns) / len(tokens) if tokens else 0\n",
        "\n",
        "# =========================\n",
        "# Emotion Features (English + Bengali)\n",
        "# =========================\n",
        "def emotion_scores(text):\n",
        "    if re.search(r'[A-Za-z]', text):  # English\n",
        "        try:\n",
        "            emo = NRCLex(text)\n",
        "            return emo.raw_emotion_scores\n",
        "        except:\n",
        "            return {}\n",
        "    else:  # Bengali\n",
        "        scores = {}\n",
        "        for word in text.split():\n",
        "            emo = bengali_emotion_lexicon.get(word)\n",
        "            if emo:\n",
        "                scores[emo] = scores.get(emo, 0) + 1\n",
        "        return scores\n",
        "\n",
        "def emotion_score_sum(text):\n",
        "    emo = emotion_scores(text)\n",
        "    pos = emo.get(\"joy\", 0) + emo.get(\"trust\", 0) + emo.get(\"anticipation\", 0)\n",
        "    neg = emo.get(\"anger\", 0) + emo.get(\"fear\", 0) + emo.get(\"sadness\", 0) + emo.get(\"disgust\", 0)\n",
        "    sui = emo.get(\"suicidal\", 0) * 2\n",
        "    return pos - (neg + sui)\n",
        "\n",
        "def sentiment_polarity_score(text):\n",
        "    score = emotion_score_sum(text)\n",
        "    return np.sign(score)\n",
        "\n",
        "def positive_word_ratio(text):\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return 0\n",
        "    pos_count = sum(1 for w in words if bengali_emotion_lexicon.get(w) in [\"joy\",\"anticipation\"])\n",
        "    return pos_count / len(words)\n",
        "\n",
        "def negative_word_ratio(text):\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return 0\n",
        "    neg_count = sum(1 for w in words if bengali_emotion_lexicon.get(w) in [\"sadness\",\"anger\",\"fear\",\"suicidal\"])\n",
        "    return neg_count / len(words)\n",
        "\n",
        "# =========================\n",
        "# Psycholinguistic Features\n",
        "# =========================\n",
        "def tense_distribution(text):\n",
        "    return {\"past\": text.count(\"à¦›à¦¿à¦²\"), \"present\": text.count(\"à¦›à¦¿\"), \"future\": text.count(\"à¦¬à§‹\")}\n",
        "def modality_count(text): return sum(text.count(w) for w in [\"à¦šà¦¾à¦‡\",\"à¦¹à¦¬à§‡\",\"à¦ªà¦¾à¦°à¦¬à§‹ à¦¨à¦¾\"])\n",
        "def cognitive_process_words(text): return sum(text.count(w) for w in [\"à¦­à¦¾à¦¬à¦¿\",\"à¦¬à¦¿à¦¶à§à¦¬à¦¾à¦¸\"])\n",
        "def mental_health_keywords(text): return sum(text.count(w) for w in [\"à¦†à¦¤à§à¦®à¦¹à¦¤à§à¦¯à¦¾\",\"à¦…à¦¬à¦¸à¦¾à¦¦\"])\n",
        "\n",
        "# =========================\n",
        "# Stylistic & Behavioral\n",
        "# =========================\n",
        "def ellipsis_count(text): return text.count(\"...\")\n",
        "def repetition_ratio(text):\n",
        "    words = text.split()\n",
        "    counts = Counter(words)\n",
        "    return sum(v for v in counts.values() if v > 1) / len(words) if words else 0\n",
        "def exclamation_count(text): return text.count(\"!\")\n",
        "def use_of_question_form(text): return text.count(\"?\")\n",
        "def typing_speed_simulation(text, time_taken=10): return word_count(text) / time_taken\n",
        "\n",
        "# =========================\n",
        "# Bengali-specific\n",
        "# =========================\n",
        "def joined_words_count(text): return sum(1 for w in text.split() if \"â€Œ\" in w)\n",
        "def conjunct_count(text): return sum(text.count(c) for c in [\"à¦¤à§à¦¤\",\"à¦œà§à¦ž\"])\n",
        "def code_mixed_ratio(text):\n",
        "    words = text.split()\n",
        "    eng_count = sum(1 for w in words if re.match(r'[A-Za-z]+', w))\n",
        "    return eng_count / len(words) if words else 0\n",
        "\n",
        "# =========================\n",
        "# N-gram Features\n",
        "# =========================\n",
        "ngram_dict = {\"à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à¦›à§‡ à¦¨à¦¾\": -1, \"à¦šà¦¾à¦ª à¦¦à¦¿à¦šà§à¦›à§‡\": -1}\n",
        "def frequent_bigrams(text):\n",
        "    tokens = text.split()\n",
        "    bigrams = list(nltk.bigrams(tokens))\n",
        "    return Counter(bigrams).most_common(5)\n",
        "def ngram_emotion_score(text):\n",
        "    score = 0\n",
        "    for k, v in ngram_dict.items():\n",
        "        if k in text:\n",
        "            score += v\n",
        "    return score\n",
        "\n",
        "# =========================\n",
        "# APPLY FEATURES TO DATAFRAME\n",
        "# =========================\n",
        "df['word_count'] = texts.apply(word_count)\n",
        "df['avg_word_length'] = texts.apply(avg_word_length)\n",
        "df['unique_word_count'] = texts.apply(unique_word_count)\n",
        "df['character_count'] = texts.apply(character_count)\n",
        "df['stopword_ratio'] = texts.apply(stopword_ratio)\n",
        "df['punctuation_count'] = texts.apply(punctuation_count)\n",
        "df['emoji_count'] = texts.apply(emoji_count)\n",
        "df['uppercase_ratio'] = texts.apply(uppercase_ratio)\n",
        "\n",
        "df['pos_distribution'] = texts.apply(pos_tag_distribution)\n",
        "df['num_sentences'] = texts.apply(number_of_sentences)\n",
        "df['sentence_var'] = texts.apply(sentence_length_variance)\n",
        "df['negation_count'] = texts.apply(use_of_negation)\n",
        "df['pronoun_ratio'] = texts.apply(personal_pronoun_ratio)\n",
        "\n",
        "df['emotion_scores'] = texts.apply(emotion_scores)\n",
        "df['emotion_score_sum'] = texts.apply(emotion_score_sum)\n",
        "df['polarity_score'] = texts.apply(sentiment_polarity_score)\n",
        "df['positive_ratio'] = texts.apply(positive_word_ratio)\n",
        "df['negative_ratio'] = texts.apply(negative_word_ratio)\n",
        "\n",
        "df['tense_dist'] = texts.apply(tense_distribution)\n",
        "df['modality_count'] = texts.apply(modality_count)\n",
        "df['cognitive_words'] = texts.apply(cognitive_process_words)\n",
        "df['mh_keywords'] = texts.apply(mental_health_keywords)\n",
        "\n",
        "df['ellipsis_count'] = texts.apply(ellipsis_count)\n",
        "df['repetition_ratio'] = texts.apply(repetition_ratio)\n",
        "df['exclamation_count'] = texts.apply(exclamation_count)\n",
        "df['question_count'] = texts.apply(use_of_question_form)\n",
        "df['typing_speed'] = texts.apply(typing_speed_simulation)\n",
        "\n",
        "df['joined_words'] = texts.apply(joined_words_count)\n",
        "df['conjunct_count'] = texts.apply(conjunct_count)\n",
        "df['code_mixed_ratio'] = texts.apply(code_mixed_ratio)\n",
        "\n",
        "df['frequent_bigrams'] = texts.apply(frequent_bigrams)\n",
        "df['ngram_emotion_score'] = texts.apply(ngram_emotion_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uGr37d9yGoD"
      },
      "source": [
        "## Describe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTKwseicyKaj"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEapnxceyOEm"
      },
      "source": [
        "## Mising value finding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5SEERV4yMp_"
      },
      "outputs": [],
      "source": [
        "(df.isnull().sum() / len(df)) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVJusbB2yV95"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOPfn6JHySUk"
      },
      "outputs": [],
      "source": [
        "\n",
        "df['Sentence'] = df['Sentence'].fillna('')\n",
        "df['Emotion'] = df['Emotion'].fillna('Neutral')\n",
        "df['Language Type'] = df['Language Type'].fillna('Unknown')\n",
        "df['User Age'] = df['User Age'].fillna(df['User Age'].median())\n",
        "df['Gender'] = df['Gender'].fillna('Other')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMm5fc6ZyYUS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# =========================\n",
        "# Convert unhashable columns to string for safe duplicate removal\n",
        "# =========================\n",
        "for col in df.columns:\n",
        "    # check first value to see if it's a dict or list\n",
        "    if len(df[col]) > 0 and isinstance(df[col].iloc[0], (dict, list)):\n",
        "        df[col] = df[col].astype(str)\n",
        "\n",
        "# =========================\n",
        "# Remove duplicates safely\n",
        "# =========================\n",
        "num_duplicates_before = len(df)\n",
        "df = df.drop_duplicates()\n",
        "num_duplicates_after = len(df)\n",
        "print(\"Number of duplicate rows removed:\", num_duplicates_before - num_duplicates_after)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sehhGF-2ydl1"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Standardize Categorical Columns\n",
        "# =========================\n",
        "df['Language Type'] = df['Language Type'].replace({'English ': 'English', 'english': 'English'})\n",
        "df['Gender'] = df['Gender'].replace({'Female ': 'Female', 'female': 'Female'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFTLsP4uygnR"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Encode Categorical Columns\n",
        "# =========================\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le_emotion = LabelEncoder()\n",
        "le_language = LabelEncoder()\n",
        "le_gender = LabelEncoder()\n",
        "\n",
        "df['Emotion_encoded'] = le_emotion.fit_transform(df['Emotion'])\n",
        "df['Language_encoded'] = le_language.fit_transform(df['Language Type'])\n",
        "df['Gender_encoded'] = le_gender.fit_transform(df['Gender'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvsJnkbvykNW"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Statistical Summary\n",
        "# =========================\n",
        "print(\"\\n--- Statistical Summary (Numerical Columns) ---\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ilIoaWlym8J"
      },
      "outputs": [],
      "source": [
        "\n",
        "# SOUMILI\n",
        "print(\"\\n--- Value Counts for 'Emotion' ---\")\n",
        "print(df['Emotion'].value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Yks9eJxytM6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# SOUMILI\n",
        "print(\"\\n--- Value Counts for 'Gender' ---\")\n",
        "print(df['Gender'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0LZ1RMFyxXa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# SOUMILI\n",
        "print(\"\\n--- Value Counts for 'Language Type' ---\")\n",
        "print(df['Language Type'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETlHI_TUyzpw"
      },
      "source": [
        "## Statistical analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0rVG6Y_yyZu"
      },
      "outputs": [],
      "source": [
        "\n",
        "#----------------------------------\n",
        "# 1. Encode categorical columns\n",
        "# ----------------------------------------------------\n",
        "le_emotion = LabelEncoder()\n",
        "df['Emotion_encoded'] = le_emotion.fit_transform(df['Emotion'])\n",
        "\n",
        "le_lang = LabelEncoder()\n",
        "df['Language_encoded'] = le_lang.fit_transform(df['Language Type'])\n",
        "\n",
        "le_gender = LabelEncoder()\n",
        "df['Gender_encoded'] = le_gender.fit_transform(df['Gender'])\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. Descriptive Statistics for numeric features\n",
        "# ----------------------------------------------------\n",
        "print(\"\\n=== Descriptive Statistics of Key Features ===\\n\")\n",
        "print(df[['word_count', 'avg_word_length', 'unique_word_count',\n",
        "          'character_count', 'stopword_ratio',\n",
        "          'emotion_score_sum', 'polarity_score',\n",
        "          'positive_ratio', 'negative_ratio',\n",
        "          'User Age']].describe())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-POaNSVCy5k3"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------\n",
        "# 3. Correlation Heatmap\n",
        "# ----------------------------------------------------\n",
        "plt.figure(figsize=(12,7))\n",
        "corr = df.corr(numeric_only=True)\n",
        "sns.heatmap(corr, cmap=\"coolwarm\", center=0, annot=False)\n",
        "plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZyadhx8y9t_"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------\n",
        "# 4. Feature Distribution by Emotion\n",
        "# ----------------------------------------------------\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x='Emotion', y='word_count', data=df)\n",
        "plt.title(\"Word Count Distribution by Emotion\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x='Emotion', y='emotion_score_sum', data=df)\n",
        "plt.title(\"Emotion Score Sum by Emotion\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC-XzKD9zDfu"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeDahtBJzF_o"
      },
      "outputs": [],
      "source": [
        "# 1. Emotion Distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x=\"Emotion\", palette=\"viridis\")\n",
        "plt.title(\"Emotion Distribution (Bar Chart)\")\n",
        "plt.xticks(rotation=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2d7GglNzLB9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "df['Emotion'].value_counts().plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    colors=sns.color_palette(\"viridis\")\n",
        ")\n",
        "plt.title(\"Emotion Distribution (Pie Chart)\")\n",
        "plt.ylabel(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq3At9yHzOxI"
      },
      "outputs": [],
      "source": [
        "#Soumili\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x=\"Language Type\", hue=\"Emotion\", palette=\"viridis\")\n",
        "plt.title(\"Language vs Emotion Distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq4Kxz6GzQqH"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.boxplot(data=df, x=\"Emotion\", y=\"word_count\", palette=\"viridis\")\n",
        "plt.title(\"Word Count Distribution per Emotion (Box Plot)\")\n",
        "plt.xticks(rotation=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsLdnWPPzSN4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.violinplot(data=df, x=\"Emotion\", y=\"word_count\", palette=\"viridis\")\n",
        "plt.title(\"Word Count Distribution per Emotion (Violin Plot)\")\n",
        "plt.xticks(rotation=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z8jBu-JzWGC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example: generate word clouds per emotion\n",
        "emotions = df['Emotion'].unique()\n",
        "\n",
        "for emo in emotions:\n",
        "    text = \" \".join(df[df['Emotion'] == emo])\n",
        "\n",
        "    # Create classic style word cloud\n",
        "    wc = WordCloud(\n",
        "        width=800, height=500,\n",
        "        background_color=\"white\",   # clean white background\n",
        "        colormap=\"viridis\",         # nice gradient colors\n",
        "        max_words=100,              # top N words\n",
        "        collocations=False,         # avoid joining common pairs\n",
        "        font_path=None              # set path if Bengali needed\n",
        "    ).generate(text)\n",
        "\n",
        "    # Show word cloud\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Word Cloud - {emo}\", fontsize=16)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfDptC2pzYhH"
      },
      "outputs": [],
      "source": [
        "plt.hist(df['word_count'], bins=5, color=\"purple\", alpha=0.7)\n",
        "plt.title(\"Text Length Distribution\")\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zHPQmeOzcTJ"
      },
      "outputs": [],
      "source": [
        "df['Language Type'].value_counts().plot(kind=\"bar\", color=\"orange\")\n",
        "plt.title(\"Language Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wVAwHJgzeca"
      },
      "outputs": [],
      "source": [
        "avg_age = df.groupby('Emotion')['User Age'].mean()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(avg_age.index, avg_age.values, marker='o', linestyle='-', linewidth=2)\n",
        "plt.title('Average User Age by Emotion')\n",
        "plt.xlabel('Emotion')\n",
        "plt.ylabel('Average Age')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO0qDrniz38D"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.hist(df['User Age'], bins=5, edgecolor='black')\n",
        "plt.title('Distribution of User Ages')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjaWi6q-1kKj"
      },
      "source": [
        "## Hybrid Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRwlkPM3z9qj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -------------------------------\n",
        "# 1. Encode Categorical Columns\n",
        "# -------------------------------\n",
        "le_lang = LabelEncoder()\n",
        "le_gender = LabelEncoder()\n",
        "\n",
        "df['Lang_Code'] = le_lang.fit_transform(df['Language Type'])\n",
        "df['Gender_Code'] = le_gender.fit_transform(df['Gender'])\n",
        "\n",
        "# -------------------------------\n",
        "# 2.Define Features (X) and Target (y)\n",
        "# -------------------------------\n",
        "X_text = df['Sentence']            # text data\n",
        "X_meta = df[['User Age', 'Lang_Code', 'Gender_Code']]  # numeric + encoded features\n",
        "y = df['Emotion']\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Train-Test Split\n",
        "# -------------------------------\n",
        "X_train_text, X_test_text, X_train_meta, X_test_meta, y_train, y_test = train_test_split(\n",
        "    X_text, X_meta, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. TF-IDF Vectorization for Text\n",
        "# -------------------------------\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
        "X_test_tfidf = vectorizer.transform(X_test_text)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Scale Numeric Features\n",
        "# -------------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_meta_scaled = scaler.fit_transform(X_train_meta)\n",
        "X_test_meta_scaled = scaler.transform(X_test_meta)\n",
        "\n",
        "# Combine TF-IDF and metadata\n",
        "X_train_combined = hstack([X_train_tfidf, X_train_meta_scaled])\n",
        "X_test_combined = hstack([X_test_tfidf, X_test_meta_scaled])\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Model Training\n",
        "# -------------------------------\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_combined, y_train)\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Model Testing\n",
        "# -------------------------------\n",
        "y_pred = model.predict(X_test_combined)\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Evaluation\n",
        "# -------------------------------\n",
        "print(\"âœ… Accuracy:\", round(accuracy_score(y_test, y_pred) * 100, 2), \"%\")\n",
        "print(\"\\nðŸ“Š Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUXL-t0Ibr3z"
      },
      "source": [
        "## Cross Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3jOWVARbsW9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import hstack\n",
        "import numpy as np\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_accuracies = []\n",
        "\n",
        "for train_idx, test_idx in cv.split(X_text, y):\n",
        "    X_train_cv_text, X_test_cv_text = X_text.iloc[train_idx], X_text.iloc[test_idx]\n",
        "    X_train_cv_meta, X_test_cv_meta = X_meta.iloc[train_idx], X_meta.iloc[test_idx]\n",
        "    y_train_cv, y_test_cv = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "    # TF-IDF\n",
        "    X_train_cv_tfidf = vectorizer.fit_transform(X_train_cv_text)\n",
        "    X_test_cv_tfidf = vectorizer.transform(X_test_cv_text)\n",
        "\n",
        "    # Scale meta features\n",
        "    X_train_cv_meta_scaled = scaler.fit_transform(X_train_cv_meta)\n",
        "    X_test_cv_meta_scaled = scaler.transform(X_test_cv_meta)\n",
        "\n",
        "    # Combine\n",
        "    X_train_cv_combined = hstack([X_train_cv_tfidf, X_train_cv_meta_scaled])\n",
        "    X_test_cv_combined = hstack([X_test_cv_tfidf, X_test_cv_meta_scaled])\n",
        "\n",
        "    # Train\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_cv_combined, y_train_cv)\n",
        "\n",
        "    # Accuracy\n",
        "    acc = model.score(X_test_cv_combined, y_test_cv)\n",
        "    cv_accuracies.append(acc)\n",
        "\n",
        "print(\"5-Fold CV Mean Accuracy:\", round(np.mean(cv_accuracies)*100, 2), \"%\")\n",
        "print(\"5-Fold CV Std Dev:\", round(np.std(cv_accuracies)*100, 2), \"%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbtcEbBv_VPr"
      },
      "source": [
        "## ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmqLoEDT1zMJ"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM\": SVC(kernel='linear', probability=True),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=3),\n",
        "    \"Multinomial Naive Bayes\": MultinomialNB()  # NB uses text only\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9xFuVx33Vd4"
      },
      "outputs": [],
      "source": [
        "for name, model_instance in models.items():\n",
        "    print(f\"\\n=== Training {name} ===\")\n",
        "\n",
        "    # MultinomialNB cannot handle scaled numeric features, so we use TF-IDF text only\n",
        "    if name == \"Multinomial Naive Bayes\":\n",
        "        model_instance.fit(X_train_tfidf, y_train)\n",
        "        y_pred = model_instance.predict(X_test_tfidf)\n",
        "    else:\n",
        "        model_instance.fit(X_train_combined, y_train)\n",
        "        y_pred = model_instance.predict(X_test_combined)\n",
        "\n",
        "    # Accuracy and Classification Report\n",
        "    print(f\"Accuracy: {round(accuracy_score(y_test, y_pred) * 100, 2)}%\")\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvuZZ7pxe5c-"
      },
      "source": [
        "## ROC-AUC CURVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESdSRjhteKl0"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "# -----------------------------\n",
        "# ROC-AUC Computation (Multi-class supported)\n",
        "# -----------------------------\n",
        "try:\n",
        "    # Get predicted probabilities for the test set\n",
        "    # For Naive Bayes, we already used TF-IDF; for others, combined features\n",
        "    if name == \"Multinomial Naive Bayes\":\n",
        "        y_pred_prob = model_instance.predict_proba(X_test_tfidf)\n",
        "    else:\n",
        "        y_pred_prob = model_instance.predict_proba(X_test_combined)\n",
        "\n",
        "    # Binarize labels for multi-class ROC-AUC\n",
        "    classes = sorted(list(set(y_test)))\n",
        "    y_test_bin = label_binarize(y_test, classes=classes)\n",
        "\n",
        "    # Compute ROC-AUC using One-vs-Rest approach\n",
        "    roc_auc = roc_auc_score(y_test_bin, y_pred_prob, multi_class='ovr')\n",
        "    print(f\"ROC-AUC Score ({name}): {roc_auc:.3f}\")\n",
        "\n",
        "    # Optional: Plot ROC for each class\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    for i in range(len(classes)):\n",
        "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_prob[:, i])\n",
        "        plt.plot(fpr, tpr, lw=2, label=f'Class {classes[i]} (AUC = {auc(fpr, tpr):.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curves - {name}')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ROC-AUC not available for {name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COxtDAeu_Z6P"
      },
      "source": [
        "## Accuracy comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myyB90EK3YXa"
      },
      "outputs": [],
      "source": [
        "model_names = []\n",
        "accuracies = []\n",
        "\n",
        "# -------------------------------\n",
        "# Train and Evaluate Models\n",
        "# -------------------------------\n",
        "for name, model_instance in models.items():\n",
        "    print(f\"\\n=== Training {name} ===\")\n",
        "\n",
        "    if name == \"Multinomial Naive Bayes\":\n",
        "        model_instance.fit(X_train_tfidf, y_train)\n",
        "        y_pred = model_instance.predict(X_test_tfidf)\n",
        "    else:\n",
        "        model_instance.fit(X_train_combined, y_train)\n",
        "        y_pred = model_instance.predict(X_test_combined)\n",
        "\n",
        "    acc = round(accuracy_score(y_test, y_pred) * 100, 2)\n",
        "    model_names.append(name)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "    # print(f\"Accuracy: {acc}%\")\n",
        "    # print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# -------------------------------\n",
        "# Visualization: Horizontal Bar Graph\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(model_names, accuracies, color='skyblue')\n",
        "plt.xlabel(\"Accuracy (%)\")\n",
        "plt.title(\"Comparison of ML Model Accuracies\")\n",
        "plt.xlim(0, 100)\n",
        "for i, v in enumerate(accuracies):\n",
        "    plt.text(v + 0.5, i, str(v) + \"%\", va='center')\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# Identify the Best Model\n",
        "# -------------------------------\n",
        "max_acc_index = accuracies.index(max(accuracies))\n",
        "best_model_name = model_names[max_acc_index]\n",
        "best_model_accuracy = accuracies[max_acc_index]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2ewhxu352X8"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nâœ… Most Accurate Model: {best_model_name} with Accuracy: {best_model_accuracy}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vdlEzig_Rz_"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nAKE5eE8n4X"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Translator: Bengali â†’ English ---\n",
        "translator = Translator()\n",
        "\n",
        "def translate_bn_to_en(text: str) -> str:\n",
        "    translated = translator.translate(text, src='bn', dest='en')\n",
        "    return translated.text\n",
        "\n",
        "# --- Emotion classifier (English) ---\n",
        "emotion_classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "    device=-1,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# --- Normalize Bengali slang/colloquial words before translation ---\n",
        "slang_map = {\n",
        "    \"furti\": \"happy\",\n",
        "    \"khushi\": \"joy\",\n",
        "    \"bhalo\": \"good\",\n",
        "    \"mon bhalo\": \"happy\",\n",
        "    \"dukho\": \"sad\",\n",
        "    \"kharap\": \"bad\",\n",
        "    \"hebi valo\": \"happy\"\n",
        "}\n",
        "\n",
        "def normalize_bengali_text(text: str) -> str:\n",
        "    t_low = text.lower()\n",
        "    for bn_word, en_word in slang_map.items():\n",
        "        t_low = t_low.replace(bn_word, en_word)\n",
        "    return t_low\n",
        "\n",
        "# --- Keywords ---\n",
        "suicidal_keywords = [\n",
        "    \"more jete iccha\", \"more jete chai\", \"more jabo\", \"marte chai\",\n",
        "    \"amar jibon sesh\", \"amar sesh\", \"jibon sesh korte chai\",\n",
        "    \"atmahatya\", \"aatmahatya\", \"suicide\", \"self harm\", \"self-harm\",\n",
        "    \"nijeke marbo\", \"nijeke sesh korbo\", \"banchte iccha nei\", \"bachte chaina\",\n",
        "    \"ar parchi na\", \"amar shesh\", \"morle bhalo hoto\", \"phasi nebo\",\n",
        "    \"bish khabo\", \"jump korbo\", \"train e kude debo\",\"i don't want to live\", \"i dont want to live\",\n",
        "    \"i can't go on\", \"i cant go on\",\n",
        "    \"life is not worth\", \"end my life\",\n",
        "    \"kill myself\", \"die\", \"i want to die\",\n",
        "    \"living anymore\", \"no reason to live\"\n",
        "]\n",
        "\n",
        "threat_keywords = [\n",
        "    \"ami toke mere felbo\", \"mere felbo\", \"ghushie debo\", \"toke katbo\",\n",
        "    \"toke goli korbo\", \"toke dhongsho korbo\", \"tor jibon sesh\", \"marbo toke\",\n",
        "    \"rokto khun\", \"pagol kore debo\", \"toke noshto kore debo\"\n",
        "]\n",
        "\n",
        "# --- Suggestions by emotion ---\n",
        "suggestion_dict = {\n",
        "    \"joy\": \"ðŸŒŸ Stay positive and continue what makes you happy!\",\n",
        "    \"sadness\": \"ðŸ’™ Itâ€™s okay to feel low sometimes, but things will improve.\",\n",
        "    \"anger\": \"ðŸ˜¡ Try calming activities like deep breathing or a walk.\",\n",
        "    \"fear\": \"âœ… Try calming activities like deep breathing or grounding yourself.\",\n",
        "    \"surprise\": \"ðŸ˜² Unexpected things happen â€” take a moment to process calmly.\",\n",
        "    \"disgust\": \"ðŸ¤¢ Try distancing yourself from whatâ€™s making you uncomfortable.\",\n",
        "    \"neutral\": \"ðŸ˜Œ Stay mindful and balanced.\",\n",
        "}\n",
        "\n",
        "# --- Extended emotion cause keywords ---\n",
        "cause_keywords = {\n",
        "    \"job\": [\"kaj\", \"job\", \"career\", \"boss\", \"promotion\", \"salary\", \"interview\", \"office\", \"co-worker\", \"assignment\"],\n",
        "    \"family\": [\"baba\", \"ma\", \"poribar\", \"bhai\", \"bon\", \"pita\", \"mata\", \"chhele\", \"meye\", \"family\", \"parents\", \"home\"],\n",
        "    \"love/relationship\": [\"prem\", \"love\", \"valobasha\", \"chele\", \"mey\", \"boyfriend\", \"girlfriend\", \"breakup\", \"dating\", \"relationship\"],\n",
        "    \"study/education\": [\"porashona\", \"exam\", \"study\", \"class\", \"assignment\", \"project\", \"results\", \"test\", \"university\", \"school\", \"college\"],\n",
        "    \"health\": [\"shorir\", \"bimar\", \"doctor\", \"hospital\", \"illness\", \"disease\", \"pain\", \"treatment\", \"sick\"],\n",
        "    \"financial\": [\"taka\", \"loan\", \"bank\", \"money\", \"expense\", \"budget\", \"financial\", \"bikri\", \"shopping\"],\n",
        "    \"friendship/social\": [\"bondhu\", \"friend\", \"friends\", \"party\", \"social\", \"meet\", \"hangout\", \"conflict\", \"betrayal\"],\n",
        "    \"self-esteem/personal\": [\"niye asha\", \"confidence\", \"motivation\", \"success\", \"failure\", \"disappointed\", \"insecure\", \"stress\"],\n",
        "    \"environmental/stress\": [\"traffic\", \"pollution\", \"noise\", \"workload\", \"pressure\", \"deadline\", \"stressful\", \"overwhelmed\"],\n",
        "    \"romantic loss/grief\": [\"bere hoye geche\", \"bere giyeche\", \"lost love\", \"heartbroken\", \"separation\", \"mourning\"],\n",
        "}\n",
        "\n",
        "# --- Session memory ---\n",
        "session_memory = []\n",
        "\n",
        "# --- Forensic Feature Extraction ---\n",
        "def extract_forensic_features(text: str) -> dict:\n",
        "    words = re.findall(r\"\\w+\", text)\n",
        "    word_count = len(words)\n",
        "    avg_word_length = sum(len(w) for w in words) / word_count if word_count else 0\n",
        "    repetition_count = sum(text.lower().split().count(w) > 1 for w in set(text.lower().split()))\n",
        "    negation_count = len(re.findall(r\"\\b(na|nai|no|not|never|don\\'t|can't|won't)\\b\", text.lower()))\n",
        "    passive_voice = len(re.findall(r\"\\b(be|was|were|been|being|is|are|am)\\s+\\w+ed\\b\", text.lower()))\n",
        "    exclamations = text.count(\"!\")\n",
        "    ellipsis = text.count(\"...\")\n",
        "    return {\n",
        "        \"word_count\": word_count,\n",
        "        \"avg_word_length\": avg_word_length,\n",
        "        \"repetition_count\": repetition_count,\n",
        "        \"negation_count\": negation_count,\n",
        "        \"passive_voice\": passive_voice,\n",
        "        \"exclamations\": exclamations,\n",
        "        \"ellipsis\": ellipsis\n",
        "    }\n",
        "\n",
        "# --- Smart Bengali sentence splitter ---\n",
        "def split_bengali_sentences(text: str):\n",
        "    # Split at ., ? ! or conjunctions like à¦•à¦¿à¦¨à§à¦¤à§, à¦à¦¬à¦‚, à¦…à¦¥à¦¬à¦¾\n",
        "    parts = re.split(r'(?<=à¥¤)|(?<=\\?)|(?<=!)|(?<=,)|\\s+à¦•à¦¿à¦¨à§à¦¤à§\\s+|\\s+à¦à¦¬à¦‚\\s+|\\s+à¦…à¦¥à¦¬à¦¾\\s+', text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "# --- Analyze function ---\n",
        "def analyze_text_multi_emotion(text: str):\n",
        "    clauses = split_bengali_sentences(text)\n",
        "    results = []\n",
        "    session_memory.clear()  # clear once at the start\n",
        "\n",
        "    for clause in clauses:\n",
        "        # Normalize slang\n",
        "        text_norm = normalize_bengali_text(clause)\n",
        "\n",
        "        # Translate safely\n",
        "        try:\n",
        "            translated = translate_bn_to_en(text_norm)\n",
        "            if not translated:\n",
        "                translated = text_norm\n",
        "        except:\n",
        "            translated = text_norm\n",
        "\n",
        "        # Emotion detection (top-3)\n",
        "        emo_res = emotion_classifier(translated, top_k=3)\n",
        "        if not emo_res:\n",
        "            continue  # skip if classifier fails\n",
        "\n",
        "        top = emo_res[0]\n",
        "        label = top[\"label\"].lower()\n",
        "        score = float(top[\"score\"])\n",
        "\n",
        "        # Intensity\n",
        "        if score >= 0.85:\n",
        "            intensity = \"high\"\n",
        "            intensity_num = 3\n",
        "        elif score >= 0.65:\n",
        "            intensity = \"medium\"\n",
        "            intensity_num = 2\n",
        "        else:\n",
        "            intensity = \"low\"\n",
        "            intensity_num = 1\n",
        "\n",
        "        # Suicide/Threat detection\n",
        "        t_low = clause.lower()\n",
        "        if any(k in t_low for k in suicidal_keywords):\n",
        "            stage2 = \"suicidal\"\n",
        "            suggestion = \"âš ï¸ Please reach out to a trusted friend or professional immediately.\"\n",
        "            risk_score = 10\n",
        "        elif any(k in t_low for k in threat_keywords):\n",
        "            stage2 = \"threat/anger\"\n",
        "            suggestion = \"âš ï¸ This text contains violent intent. Stay safe and alert authorities if needed.\"\n",
        "            risk_score = 8\n",
        "        else:\n",
        "            stage2 = \"not suicidal\"\n",
        "            suggestion = suggestion_dict.get(label, \"ðŸ’¡ Take care of yourself, and stay mindful.\")\n",
        "            risk_score = intensity_num * 2 if label in [\"sadness\", \"anger\", \"fear\"] else intensity_num\n",
        "\n",
        "        # Multi-cause detection\n",
        "        causes_detected = [cause for cause, keywords in cause_keywords.items() if any(k in t_low for k in keywords)]\n",
        "        if not causes_detected:\n",
        "            causes_detected = [\"unknown\"]\n",
        "\n",
        "        # Forensics\n",
        "        forensics = extract_forensic_features(clause)\n",
        "\n",
        "        # Store session\n",
        "        session_memory.append({\n",
        "            \"sentence\": clause,\n",
        "            \"label\": label,\n",
        "            \"score\": round(score, 3),\n",
        "            \"Intensity\": intensity,\n",
        "            \"stage2\": stage2,\n",
        "            \"risk_score\": risk_score,\n",
        "            \"causes\": causes_detected,\n",
        "            \"suggestion\": suggestion,\n",
        "            \"Forensics\": forensics,\n",
        "            \"Top_3_Emotions\": emo_res\n",
        "        })\n",
        "\n",
        "        # Prepare output\n",
        "        results.append({\n",
        "            \"Sentence\": clause,\n",
        "            \"Emotion\": label,\n",
        "            \"Accuracy\": f\"{score*100:.2f}%\",\n",
        "            \"Suggestion\": suggestion\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJAQ26EL_oXm"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import json\n",
        "from transformers import pipeline\n",
        "\n",
        "# --- Zero-shot classifier using IndicBERT multilingual model ---\n",
        "emotion_classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device=-1  # CPU\n",
        ")\n",
        "\n",
        "# --- Define emotion labels ---\n",
        "emotion_labels = [\"joy\", \"sadness\", \"anger\", \"fear\", \"surprise\", \"neutral\"]\n",
        "\n",
        "# --- Normalize Bengali slang ---\n",
        "slang_map = {\n",
        "    \"furti\": \"happy\",\n",
        "    \"khushi\": \"joy\",\n",
        "    \"bhalo\": \"good\",\n",
        "    \"mon bhalo\": \"happy\",\n",
        "    \"dukho\": \"sad\",\n",
        "    \"kharap\": \"bad\"\n",
        "}\n",
        "\n",
        "def normalize_bengali_text(text: str) -> str:\n",
        "    t_low = text.lower()\n",
        "    for bn_word, en_word in slang_map.items():\n",
        "        t_low = t_low.replace(bn_word, en_word)\n",
        "    return t_low\n",
        "\n",
        "# --- Keywords for risk assessment ---\n",
        "suicidal_keywords = [\"marte chai\", \"amar jibon sesh\", \"suicide\", \"atmahatya\", \"kill myself\", \"i want to die\"]\n",
        "threat_keywords = [\"toke goli korbo\", \"tor jibon sesh\", \"pagol kore debo\"]\n",
        "\n",
        "# --- Suggestions by emotion ---\n",
        "suggestion_dict = {\n",
        "    \"joy\": \"ðŸŒŸ Stay positive and continue what makes you happy!\",\n",
        "    \"sadness\": \"ðŸ’™ Itâ€™s okay to feel low sometimes, but things will improve.\",\n",
        "    \"anger\": \"ðŸ˜¡ Try calming activities like deep breathing or a walk.\",\n",
        "    \"fear\": \"âœ… Try calming activities like deep breathing or grounding yourself.\",\n",
        "    \"surprise\": \"ðŸ˜² Unexpected things happen â€” take a moment to process calmly.\",\n",
        "    \"neutral\": \"ðŸ˜Œ Stay mindful and balanced.\",\n",
        "}\n",
        "\n",
        "# --- Cause keywords ---\n",
        "cause_keywords = {\n",
        "    \"job\": [\"kaj\", \"job\", \"career\", \"boss\", \"promotion\", \"salary\", \"interview\", \"office\"],\n",
        "    \"family\": [\"baba\", \"ma\", \"poribar\", \"bhai\", \"bon\", \"family\", \"parents\"],\n",
        "    \"love/relationship\": [\"prem\", \"love\", \"chele\", \"mey\", \"boyfriend\", \"girlfriend\", \"breakup\", \"relationship\"],\n",
        "    \"study/education\": [\"porashona\", \"exam\", \"study\", \"class\", \"assignment\", \"project\", \"results\", \"school\", \"college\"],\n",
        "    \"health\": [\"shorir\", \"bimar\", \"doctor\", \"hospital\", \"illness\", \"pain\"],\n",
        "    \"financial\": [\"taka\", \"loan\", \"bank\", \"money\", \"expense\", \"budget\"],\n",
        "    \"friendship/social\": [\"bondhu\", \"friend\", \"friends\", \"party\", \"social\", \"meet\"],\n",
        "    \"self-esteem/personal\": [\"confidence\", \"motivation\", \"success\", \"failure\", \"stress\"],\n",
        "    \"environmental/stress\": [\"traffic\", \"pollution\", \"noise\", \"pressure\", \"deadline\", \"overwhelmed\"],\n",
        "    \"romantic loss/grief\": [\"bere hoye geche\", \"lost love\", \"heartbroken\", \"separation\", \"mourning\"],\n",
        "}\n",
        "\n",
        "# --- Session memory ---\n",
        "session_memory = []\n",
        "\n",
        "# --- Forensic feature extraction ---\n",
        "def extract_forensic_features(text: str) -> dict:\n",
        "    words = re.findall(r\"\\w+\", text)\n",
        "    word_count = len(words)\n",
        "    avg_word_length = sum(len(w) for w in words) / word_count if word_count else 0\n",
        "    repetition_count = sum(text.lower().split().count(w) > 1 for w in set(text.lower().split()))\n",
        "    negation_count = len(re.findall(r\"\\b(na|nai|no|not|never|don\\'t|can't|won't)\\b\", text.lower()))\n",
        "    passive_voice = len(re.findall(r\"\\b(be|was|were|been|being|is|are|am)\\s+\\w+ed\\b\", text.lower()))\n",
        "    exclamations = text.count(\"!\")\n",
        "    ellipsis = text.count(\"...\")\n",
        "    return {\n",
        "        \"word_count\": word_count,\n",
        "        \"avg_word_length\": avg_word_length,\n",
        "        \"repetition_count\": repetition_count,\n",
        "        \"negation_count\": negation_count,\n",
        "        \"passive_voice\": passive_voice,\n",
        "        \"exclamations\": exclamations,\n",
        "        \"ellipsis\": ellipsis\n",
        "    }\n",
        "\n",
        "# --- Bengali clause splitter (fixed) ---\n",
        "def split_bengali_clauses(text: str):\n",
        "    clauses = re.split(r'\\s*(?:,|à¦•à¦¿à¦¨à§à¦¤à§|à¦¤à¦¬à§‡|à¦à¦¬à¦‚|à¦†à¦°|à¦¬à¦¾)\\s*', text)\n",
        "    return [c.strip() for c in clauses if c.strip()]\n",
        "\n",
        "def analyze_text_advanced(text: str):\n",
        "    clauses = split_bengali_clauses(text)\n",
        "    results = []\n",
        "    session_memory.clear()  # clear memory at the start\n",
        "\n",
        "    for clause in clauses:\n",
        "        if not clause.strip():\n",
        "            continue  # skip empty clauses\n",
        "\n",
        "        text_norm = normalize_bengali_text(clause)\n",
        "\n",
        "        # âœ… Zero-shot classification safely\n",
        "        try:\n",
        "            res = emotion_classifier(\n",
        "                sequences=text_norm,\n",
        "                labels=emotion_labels,\n",
        "                hypothesis_template=\"This text expresses {}.\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            # fallback if classifier fails\n",
        "            res = {\"labels\": [\"neutral\"], \"scores\": [1.0]}\n",
        "\n",
        "        label = res['labels'][0]\n",
        "        score = res['scores'][0]\n",
        "\n",
        "        # Intensity\n",
        "        if score >= 0.85:\n",
        "            intensity = \"high\"\n",
        "            intensity_num = 3\n",
        "        elif score >= 0.65:\n",
        "            intensity = \"medium\"\n",
        "            intensity_num = 2\n",
        "        else:\n",
        "            intensity = \"low\"\n",
        "            intensity_num = 1\n",
        "\n",
        "        # Risk assessment\n",
        "        t_low = clause.lower()\n",
        "        if any(k in t_low for k in suicidal_keywords):\n",
        "            stage2 = \"suicidal\"\n",
        "            suggestion = \"âš ï¸ Please reach out to a trusted friend or professional immediately.\"\n",
        "            risk_score = 10\n",
        "        elif any(k in t_low for k in threat_keywords):\n",
        "            stage2 = \"threat/anger\"\n",
        "            suggestion = \"âš ï¸ This text contains violent intent. Stay safe and alert authorities if needed.\"\n",
        "            risk_score = 8\n",
        "        else:\n",
        "            stage2 = \"not suicidal\"\n",
        "            suggestion = suggestion_dict.get(label, \"ðŸ’¡ Take care of yourself.\")\n",
        "            risk_score = intensity_num * 2 if label in [\"sadness\", \"anger\", \"fear\"] else intensity_num\n",
        "\n",
        "        # Cause detection\n",
        "        causes_detected = [cause for cause, keywords in cause_keywords.items() if any(k in t_low for k in keywords)]\n",
        "        if not causes_detected:\n",
        "            causes_detected = [\"unknown\"]\n",
        "\n",
        "        # Forensics\n",
        "        forensics = extract_forensic_features(clause)\n",
        "\n",
        "        # Store session\n",
        "        session_memory.append({\n",
        "            \"sentence\": clause,\n",
        "            \"label\": label,\n",
        "            \"score\": round(score, 3),\n",
        "            \"Intensity\": intensity,\n",
        "            \"intensity_num\": intensity_num,\n",
        "            \"stage2\": stage2,\n",
        "            \"risk_score\": risk_score,\n",
        "            \"causes\": causes_detected,\n",
        "            \"suggestion\": suggestion,\n",
        "            \"Forensics\": forensics\n",
        "        })\n",
        "\n",
        "        # Prepare output\n",
        "        top_3 = [{\"Emotion\": res['labels'][i], \"Confidence\": f\"{res['scores'][i]*100:.2f}%\"} for i in range(min(3, len(res['labels'])))]\n",
        "        results.append({\n",
        "            \"Sentence\": clause,\n",
        "            \"Emotion\": label,\n",
        "            \"Accuracy\": f\"{score*100:.2f}%\",\n",
        "            \"Suggestion\": suggestion\n",
        "        })\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhRo9-jQ_vBB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from transformers import pipeline\n",
        "from googletrans import Translator\n",
        "\n",
        "# --- Translator: Bengali â†’ English ---\n",
        "translator = Translator()\n",
        "\n",
        "def translate_bn_to_en(text: str) -> str:\n",
        "    translated = translator.translate(text, src='bn', dest='en')\n",
        "    return translated.text\n",
        "\n",
        "# --- Emotion classifier (XLNet-based) ---\n",
        "from transformers import pipeline\n",
        "\n",
        "# XLNet emotion model\n",
        "emotion_classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"sherelyn912/emotional-xlnet\",\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# --- Normalize Bengali slang/colloquial words before translation ---\n",
        "slang_map = {\n",
        "    \"furti\": \"happy\",\n",
        "    \"khushi\": \"joy\",\n",
        "    \"bhalo\": \"good\",\n",
        "    \"mon bhalo\": \"happy\",\n",
        "    \"dukho\": \"sad\",\n",
        "    \"mon kemon\":\"sad\",\n",
        "    \"kharap\": \"bad\",\n",
        "    \"hebi valo\": \"happy\"\n",
        "}\n",
        "\n",
        "def normalize_bengali_text(text: str) -> str:\n",
        "    t_low = text.lower()\n",
        "    for bn_word, en_word in slang_map.items():\n",
        "        t_low = t_low.replace(bn_word, en_word)\n",
        "    return t_low\n",
        "\n",
        "# --- Keywords for suicide/threat ---\n",
        "suicidal_keywords = [\n",
        "    \"more jete iccha\", \"more jete chai\", \"more jabo\", \"marte chai\",\n",
        "    \"amar jibon sesh\", \"amar sesh\", \"jibon sesh korte chai\",\n",
        "    \"atmahatya\", \"aatmahatya\", \"suicide\", \"self harm\", \"self-harm\",\n",
        "    \"nijeke marbo\", \"nijeke sesh korbo\", \"banchte iccha nei\", \"bachte chaina\",\n",
        "    \"ar parchi na\", \"amar shesh\", \"morle bhalo hoto\", \"phasi nebo\",\n",
        "    \"bish khabo\", \"jump korbo\", \"train e kude debo\",\"i don't want to live\", \"i dont want to live\",\n",
        "    \"i can't go on\", \"i cant go on\",\n",
        "    \"life is not worth\", \"end my life\",\n",
        "    \"kill myself\", \"die\", \"i want to die\",\n",
        "    \"living anymore\", \"no reason to live\"\n",
        "]\n",
        "\n",
        "threat_keywords = [\n",
        "    \"ami toke mere felbo\", \"mere felbo\", \"ghushie debo\", \"toke katbo\",\n",
        "    \"toke goli korbo\", \"toke dhongsho korbo\", \"tor jibon sesh\", \"marbo toke\",\n",
        "    \"rokto khun\", \"pagol kore debo\", \"toke noshto kore debo\"\n",
        "]\n",
        "\n",
        "# --- Suggestions by emotion ---\n",
        "suggestion_dict = {\n",
        "    \"joy\": \"ðŸŒŸ Stay positive and continue what makes you happy!\",\n",
        "    \"sadness\": \"ðŸ’™ Itâ€™s okay to feel low sometimes, but things will improve.\",\n",
        "    \"anger\": \"ðŸ˜¡ Try calming activities like deep breathing or a walk.\",\n",
        "    \"fear\": \"âœ… Try calming activities like deep breathing or grounding yourself.\",\n",
        "    \"surprise\": \"ðŸ˜² Unexpected things happen â€” take a moment to process calmly.\",\n",
        "    \"disgust\": \"ðŸ¤¢ Try distancing yourself from whatâ€™s making you uncomfortable.\",\n",
        "    \"neutral\": \"ðŸ˜Œ Stay mindful and balanced.\",\n",
        "}\n",
        "\n",
        "# --- Cause keywords (same as before) ---\n",
        "cause_keywords = {\n",
        "    \"job\": [\"kaj\", \"job\", \"career\", \"boss\", \"promotion\", \"salary\", \"interview\", \"office\", \"co-worker\", \"assignment\"],\n",
        "    \"family\": [\"baba\", \"ma\", \"poribar\", \"bhai\", \"bon\", \"pita\", \"mata\", \"chhele\", \"meye\", \"family\", \"parents\", \"home\"],\n",
        "    \"love/relationship\": [\"prem\", \"love\", \"valobasha\", \"chele\", \"mey\", \"boyfriend\", \"girlfriend\", \"breakup\", \"dating\", \"relationship\"],\n",
        "    \"study/education\": [\"porashona\", \"exam\", \"study\", \"class\", \"assignment\", \"project\", \"results\", \"test\", \"university\", \"school\", \"college\"],\n",
        "    \"health\": [\"shorir\", \"bimar\", \"doctor\", \"hospital\", \"illness\", \"disease\", \"pain\", \"treatment\", \"sick\"],\n",
        "    \"financial\": [\"taka\", \"loan\", \"bank\", \"money\", \"expense\", \"budget\", \"financial\", \"bikri\", \"shopping\"],\n",
        "    \"friendship/social\": [\"bondhu\", \"friend\", \"friends\", \"party\", \"social\", \"meet\", \"hangout\", \"conflict\", \"betrayal\"],\n",
        "    \"self-esteem/personal\": [\"niye asha\", \"confidence\", \"motivation\", \"success\", \"failure\", \"disappointed\", \"insecure\", \"stress\"],\n",
        "    \"environmental/stress\": [\"traffic\", \"pollution\", \"noise\", \"workload\", \"pressure\", \"deadline\", \"stressful\", \"overwhelmed\"],\n",
        "    \"romantic loss/grief\": [\"bere hoye geche\", \"bere giyeche\", \"lost love\", \"heartbroken\", \"separation\", \"mourning\"],\n",
        "}\n",
        "\n",
        "# --- Forensic feature extraction (same as before) ---\n",
        "def extract_forensic_features(text: str) -> dict:\n",
        "    words = re.findall(r\"\\w+\", text)\n",
        "    word_count = len(words)\n",
        "    avg_word_length = sum(len(w) for w in words) / word_count if word_count else 0\n",
        "    repetition_count = sum(text.lower().split().count(w) > 1 for w in set(text.lower().split()))\n",
        "    negation_count = len(re.findall(r\"\\b(na|nai|no|not|never|don\\'t|can't|won't)\\b\", text.lower()))\n",
        "    passive_voice = len(re.findall(r\"\\b(be|was|were|been|being|is|are|am)\\s+\\w+ed\\b\", text.lower()))\n",
        "    exclamations = text.count(\"!\")\n",
        "    ellipsis = text.count(\"...\")\n",
        "    return {\n",
        "        \"word_count\": word_count,\n",
        "        \"avg_word_length\": avg_word_length,\n",
        "        \"repetition_count\": repetition_count,\n",
        "        \"negation_count\": negation_count,\n",
        "        \"passive_voice\": passive_voice,\n",
        "        \"exclamations\": exclamations,\n",
        "        \"ellipsis\": ellipsis\n",
        "    }\n",
        "\n",
        "# --- Bengali sentence splitter ---\n",
        "def split_bengali_sentences(text: str):\n",
        "    parts = re.split(r'(?<=à¥¤)|(?<=\\?)|(?<=!)|(?<=,)|\\s+à¦•à¦¿à¦¨à§à¦¤à§\\s+|\\s+à¦à¦¬à¦‚\\s+|\\s+à¦…à¦¥à¦¬à¦¾\\s+', text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "# --- Main analysis (unchanged, but now uses XLNet) ---\n",
        "session_memory = []\n",
        "\n",
        "def analyze_text_multi_emotion(text: str):\n",
        "    clauses = split_bengali_sentences(text)\n",
        "    results = []\n",
        "    session_memory.clear()\n",
        "    for clause in clauses:\n",
        "        text_norm = normalize_bengali_text(clause)\n",
        "        translated = translate_bn_to_en(text_norm)\n",
        "        emo_res = emotion_classifier(translated)[0]\n",
        "        top = max(emo_res, key=lambda x: x[\"score\"])\n",
        "        label = top[\"label\"].lower()\n",
        "        score = float(top[\"score\"])\n",
        "\n",
        "        # Emotion intensity\n",
        "        if score >= 0.85: intensity, intensity_num = \"high\", 3\n",
        "        elif score >= 0.65: intensity, intensity_num = \"medium\", 2\n",
        "        else: intensity, intensity_num = \"low\", 1\n",
        "\n",
        "        # Suicide/Threat check\n",
        "        t_low = clause.lower()\n",
        "        if any(k in t_low for k in suicidal_keywords):\n",
        "            stage2 = \"suicidal\"\n",
        "            suggestion = \"âš ï¸ Please reach out to a trusted friend or professional immediately.\"\n",
        "            risk_score = 10\n",
        "        elif any(k in t_low for k in threat_keywords):\n",
        "            stage2 = \"threat/anger\"\n",
        "            suggestion = \"âš ï¸ Violent intent detected. Stay safe and alert authorities if needed.\"\n",
        "            risk_score = 8\n",
        "        else:\n",
        "            stage2 = \"not suicidal\"\n",
        "            suggestion = suggestion_dict.get(label, \"ðŸ’¡ Take care of yourself, and stay mindful.\")\n",
        "            risk_score = intensity_num * 2 if label in [\"sadness\", \"anger\", \"fear\"] else intensity_num\n",
        "\n",
        "        # Causes\n",
        "        causes_detected = []\n",
        "        for cause, keywords in cause_keywords.items():\n",
        "            if any(k in t_low for k in keywords):\n",
        "                causes_detected.append(cause)\n",
        "        if not causes_detected:\n",
        "            causes_detected.append(\"unknown\")\n",
        "\n",
        "        forensics = extract_forensic_features(clause)\n",
        "\n",
        "        session_memory.append({\n",
        "            \"sentence\": clause,\n",
        "            \"label\": label,\n",
        "            \"score\": round(score, 3),\n",
        "            \"Intensity\": intensity,\n",
        "            \"stage2\": stage2,\n",
        "            \"risk_score\": risk_score,\n",
        "            \"causes\": causes_detected,\n",
        "            \"suggestion\": suggestion,\n",
        "            \"Forensics\": forensics,\n",
        "            \"Top_3_Emotions\": emo_res\n",
        "        })\n",
        "\n",
        "        results.append({\n",
        "            \"Sentence\": clause,\n",
        "            \"Emotion\": label,\n",
        "            \"Accuracy\": f\"{score*100:.2f}%\",\n",
        "            \"Suggestion\": suggestion,\n",
        "        })\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3ojE5_fAAS5"
      },
      "outputs": [],
      "source": [
        "\n",
        "#===============================\n",
        "# Bengali / English Emotion Analyzer with Advanced Visualizations\n",
        "# ===============================\n",
        "\n",
        "from transformers import pipeline\n",
        "from googletrans import Translator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# --- Translator (Bengali â†’ English) ---\n",
        "translator = Translator()\n",
        "def translate_bn_to_en(text: str) -> str:\n",
        "    try:\n",
        "        return translator.translate(text, src='bn', dest='en').text\n",
        "    except Exception:\n",
        "        return text\n",
        "\n",
        "# --- Emotion Models ---\n",
        "emotion_classifier_en = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", device=-1, truncation=True)\n",
        "emotion_classifier_xnli = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\", device=-1)\n",
        "emotion_classifier_xlnet = pipeline(\"text-classification\", model=\"sherelyn912/emotional-xlnet\", return_all_scores=True, device=-1)\n",
        "\n",
        "emotion_labels = [\"joy\", \"sadness\", \"anger\", \"fear\", \"disgust\", \"surprise\", \"neutral\"]\n",
        "xlnet_label_map = {\n",
        "    \"grief\": \"sadness\", \"love\": \"joy\", \"optimism\": \"joy\", \"remorse\": \"sadness\",\n",
        "    \"anger\": \"anger\", \"fear\": \"fear\", \"surprise\": \"surprise\", \"disgust\": \"disgust\", \"neutral\": \"neutral\"\n",
        "}\n",
        "\n",
        "# --- Analyzer Functions ---\n",
        "def analyze_text_multi_emotion(text: str):\n",
        "    translated = translate_bn_to_en(text)\n",
        "    result = emotion_classifier_en(translated)[0]\n",
        "    return {\n",
        "        \"Model\": \"Multi-Emotion (EN model)\",\n",
        "        \"Scores\": {result['label']: result['score']},  # only top emotion available\n",
        "        \"Accuracy\": result['score'],\n",
        "        \"Sentence\": text,\n",
        "        \"Emotion\": result['label']\n",
        "    }\n",
        "\n",
        "def analyze_text_advanced(text: str):\n",
        "    res = emotion_classifier_xnli(text, emotion_labels)\n",
        "    # Find the top emotion and its score\n",
        "    top_emotion_index = res[\"scores\"].index(max(res[\"scores\"]))\n",
        "    top_emotion_label = res[\"labels\"][top_emotion_index]\n",
        "    top_emotion_score = res[\"scores\"][top_emotion_index]\n",
        "\n",
        "    return {\n",
        "        \"Model\": \"XNLI Zero-Shot\",\n",
        "        \"Scores\": dict(zip(res[\"labels\"], res[\"scores\"])),\n",
        "        \"Accuracy\": top_emotion_score,\n",
        "        \"Sentence\": text,\n",
        "        \"Emotion\": top_emotion_label\n",
        "    }\n",
        "\n",
        "def analyze_text_xlnet(text: str):\n",
        "    translated = translate_bn_to_en(text)\n",
        "    try:\n",
        "        emo_res = emotion_classifier_xlnet(translated)[0]\n",
        "        score_dict = {xlnet_label_map.get(x[\"label\"].lower(), x[\"label\"].lower()): x[\"score\"] for x in emo_res}\n",
        "        # Find the top emotion and its score from the mapped scores\n",
        "        top_emotion_label = max(score_dict, key=score_dict.get)\n",
        "        top_emotion_score = score_dict[top_emotion_label]\n",
        "\n",
        "        return {\n",
        "            \"Model\": \"XLNet\",\n",
        "            \"Scores\": score_dict,\n",
        "            \"Accuracy\": top_emotion_score,\n",
        "            \"Sentence\": text,\n",
        "            \"Emotion\": top_emotion_label\n",
        "        }\n",
        "    except Exception:\n",
        "        return {\n",
        "            \"Model\": \"XLNet\",\n",
        "            \"Scores\": {label: 0 for label in emotion_labels},\n",
        "            \"Accuracy\": 0,\n",
        "            \"Sentence\": text,\n",
        "            \"Emotion\": \"unknown\"\n",
        "        }\n",
        "\n",
        "\n",
        "# --- Main execution outside the if block ---\n",
        "user_text = input(\"Enter a Bengali or English sentence: \")\n",
        "\n",
        "# Get results from all models\n",
        "res_multi = analyze_text_multi_emotion(user_text)\n",
        "res_adv = analyze_text_advanced(user_text)\n",
        "res_xlnet = analyze_text_xlnet(user_text)\n",
        "\n",
        "results = [res_multi, res_adv, res_xlnet]\n",
        "\n",
        "# Print results\n",
        "for res in results:\n",
        "    top_emotion = max(res[\"Scores\"], key=res[\"Scores\"].get)\n",
        "    print(f\"\\n--- {res['Model']} ---\")\n",
        "    print(f\"Top Emotion: {top_emotion}, Probability: {res['Scores'][top_emotion]*100:.2f}%\")\n",
        "summary_dict = {}\n",
        "\n",
        "for res in results:\n",
        "    top_emotion = max(res[\"Scores\"], key=res[\"Scores\"].get)\n",
        "    accuracy = res[\"Scores\"][top_emotion]  # top emotion probability\n",
        "    summary_dict[res[\"Model\"]] = {\n",
        "        \"Top Emotion\": top_emotion,\n",
        "        \"Accuracy\": accuracy\n",
        "    }\n",
        "\n",
        "# # Print the dictionary\n",
        "# print(\"\\nSummary Dictionary:\")\n",
        "# for model, info in summary_dict.items():\n",
        "#     print(f\"{model}: Top Emotion = {info['Top Emotion']}, Accuracy = {info['Accuracy']*100:.2f}%\")\n",
        "# --- 1. Accuracy Bar Chart ---\n",
        "labels = [r[\"Model\"] for r in results]\n",
        "accuracies = [r[\"Accuracy\"]*100 for r in results]\n",
        "plt.figure(figsize=(8,6))\n",
        "bars = plt.bar(labels, accuracies, color=[\"skyblue\", \"salmon\", \"lightgreen\"])\n",
        "plt.ylim(0,100)\n",
        "plt.ylabel(\"Top Emotion Probability (%)\")\n",
        "plt.title(f\"Top Emotion Probability Comparison\\nSentence: {user_text}\")\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x()+bar.get_width()/2, bar.get_height()+1, f\"{acc:.2f}%\", ha='center')\n",
        "plt.show()\n",
        "\n",
        "# --- 2. Stacked Bar Chart ---\n",
        "df_stack = pd.DataFrame({r[\"Model\"]: [r[\"Scores\"].get(e,0) for e in emotion_labels] for r in results}, index=emotion_labels)\n",
        "df_stack = df_stack.T\n",
        "df_stack.plot(kind='bar', stacked=True, figsize=(10,6), colormap='tab20')\n",
        "plt.ylabel(\"Probability\")\n",
        "plt.title(f\"Emotion Distribution per Model\\nSentence: {user_text}\")\n",
        "plt.legend(title=\"Emotion\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# --- 3. Heatmap ---\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.heatmap(df_stack, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
        "plt.title(f\"Emotion Probability Heatmap\\nSentence: {user_text}\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.xlabel(\"Emotion\")\n",
        "plt.show()\n",
        "\n",
        "# --- 4. Interactive Radar Chart (Plotly) ---\n",
        "fig = go.Figure()\n",
        "for res in results:\n",
        "    values = [res[\"Scores\"].get(e,0) for e in emotion_labels]\n",
        "    values += values[:1]  # close loop\n",
        "    fig.add_trace(go.Scatterpolar(\n",
        "        r=values,\n",
        "        theta=emotion_labels + [emotion_labels[0]],\n",
        "        fill='toself',\n",
        "        name=res[\"Model\"]\n",
        "    ))\n",
        "fig.update_layout(\n",
        "    polar=dict(radialaxis=dict(visible=True, range=[0,1])),\n",
        "    title=f\"Emotion Probability Radar Chart\\nSentence: {user_text}\",\n",
        "    showlegend=True\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9wWSdLLAXjZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP86r_k0LBGX"
      },
      "source": [
        "## Hybrid aproach(Random forest + multimodel+ xnli zero shot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui3gcteEAbYq"
      },
      "outputs": [],
      "source": [
        "def ensemble_predict(text, ml_model, ml_features, multi_model_fn, xnli_model_fn, emotion_labels):\n",
        "    # --- ML Model Probabilities ---\n",
        "    ml_pred_probs = ml_model.predict_proba(ml_features.getrow(0))[0]\n",
        "    ml_probs = {label: ml_pred_probs[i] for i, label in enumerate(ml_model.classes_)}\n",
        "    # Ensure all emotion_labels are included\n",
        "    for label in emotion_labels:\n",
        "        if label not in ml_probs:\n",
        "            ml_probs[label] = 0.0\n",
        "\n",
        "    # --- Transformer Model Probabilities ---\n",
        "    multi_res = multi_model_fn(text)\n",
        "    xnli_res = xnli_model_fn(text)\n",
        "    multi_probs = multi_res[\"Scores\"]\n",
        "    xnli_probs = xnli_res[\"Scores\"]\n",
        "\n",
        "    # --- Weighted Combination ---\n",
        "    combined_probs = {}\n",
        "    for label in emotion_labels:\n",
        "        combined_probs[label] = (\n",
        "            0.4 * ml_probs.get(label, 0) +\n",
        "            0.3 * multi_probs.get(label, 0) +\n",
        "            0.3 * xnli_probs.get(label, 0)\n",
        "        )\n",
        "\n",
        "    # --- Bengali Keyword Boost ---\n",
        "    bengali_keywords = {\n",
        "        \"à¦¦à§à¦–\": \"sadness\", \"à¦¬à§‡à¦¦à¦¨à¦¾\": \"sadness\", \"à¦•à¦¾à¦à¦¦\": \"sadness\",\n",
        "        \"à¦­à¦¾à¦²à¦¬à¦¾à¦¸à¦¾\": \"joy\", \"à¦ªà§à¦°à§‡à¦®\": \"joy\", \"à¦†à¦¶à¦¾\": \"joy\",\n",
        "        \"à¦šà¦²à§‹\": \"joy\", \"à¦à¦•à¦²à¦¾\": \"joy\",\n",
        "        \"à¦­à§Ÿ\": \"fear\", \"à¦°à¦¾à¦—\": \"anger\", \"à¦˜à§ƒà¦£à¦¾\": \"disgust\"\n",
        "    }\n",
        "    for word, emo in bengali_keywords.items():\n",
        "        if word in text:\n",
        "            combined_probs[emo] += 0.1\n",
        "\n",
        "    # --- Normalize ---\n",
        "    total = sum(combined_probs.values())\n",
        "    if total > 0:\n",
        "        for label in combined_probs:\n",
        "            combined_probs[label] /= total\n",
        "\n",
        "    # --- Final Prediction ---\n",
        "    final_emotion = max(combined_probs, key=combined_probs.get)\n",
        "    return final_emotion, combined_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkHC1w91bBU1"
      },
      "outputs": [],
      "source": [
        "#  Find best model result\n",
        "#SOUMILI\n",
        "print(user_text)\n",
        "print(\"Transformer Output::\")\n",
        "print(\"===========================================================\")\n",
        "results = [res_multi, res_adv, res_xlnet]\n",
        "best = max(results, key=lambda x: x[\"Accuracy\"])\n",
        "print(\"\\n Final Prediction (Best Model)\")\n",
        "print(f\"Sentence: {best['Sentence']}\")\n",
        "print(f\"Model: {best['Model']}\")\n",
        "print(f\"Emotion: {best['Emotion']} ({best['Accuracy']:.2f})\")\n",
        "\n",
        "print(\"===============================================================\")\n",
        "print(\"ML models output with the combination of transformer::\")\n",
        "print(\"===============================================================\")\n",
        "best_ml_model = RandomForestClassifier()\n",
        "best_ml_model.fit(X_train_combined, y_train)\n",
        "final_emotion, combined_probs = ensemble_predict(\n",
        "    user_text,\n",
        "    ml_model=best_ml_model,\n",
        "    ml_features=X_test_combined,\n",
        "    multi_model_fn=analyze_text_multi_emotion,\n",
        "    xnli_model_fn=analyze_text_advanced,\n",
        "    emotion_labels=emotion_labels  # <-- add this\n",
        ")\n",
        "\n",
        "print(\"===============================================================\")\n",
        "print(\"ML models output with the combination of transformer::\")\n",
        "print(f\"\\n Ensemble Prediction: {final_emotion}\")\n",
        "print(\"Probability Distribution:\")\n",
        "for emo, prob in combined_probs.items():\n",
        "    print(f\"  {emo}: {prob:.3f}\")\n",
        "print(\"===============================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR8U_EFcKiq5"
      },
      "outputs": [],
      "source": [
        "print(combined_probs)\n",
        "print(summary_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U1Z7O7AR3-B"
      },
      "source": [
        "## Chart representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdzyJGQRLK6P"
      },
      "outputs": [],
      "source": [
        "true_label=final_emotion\n",
        "\n",
        "# Add ensemble row dynamically\n",
        "ensemble_top = max(combined_probs, key=combined_probs.get)\n",
        "summary_dict['Ensemble'] = {'Top Emotion': ensemble_top, 'Confidence': combined_probs[ensemble_top]}\n",
        "\n",
        "# Build metrics table\n",
        "rows = []\n",
        "for model, metrics in summary_dict.items():\n",
        "    y_true = [true_label]\n",
        "    y_pred = [metrics['Top Emotion']]\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    # ROC-AUC: use probability assigned to true class from either model confidence or combined_probs\n",
        "    # ROC-AUC: use probability assigned to true class for all models\n",
        "    if model == 'Ensemble':\n",
        "        roc_auc = combined_probs.get(true_label, 0.0)\n",
        "    else:\n",
        "        # use model confidence if top emotion is the true label, else 0\n",
        "        roc_auc = metrics.get('Confidence', 0.0) if metrics['Top Emotion'] == true_label else 0.0\n",
        "\n",
        "\n",
        "    rows.append({\n",
        "        'Model': model,\n",
        "        'Top Emotion': metrics['Top Emotion'],\n",
        "        'Accuracy': acc,\n",
        "        'Precision': prec,\n",
        "        'Recall': rec,\n",
        "        'F1 Score': f1,\n",
        "        'ROC-AUC': roc_auc\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Print results\n",
        "print(\"Input::\", user_text)\n",
        "print(\"\\n======================Model metrics chart=======================\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SEe9eG4F2Pr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------- VISUALIZATION FOR ALL METRICS -------------------------\n",
        "\n",
        "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"]\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "for metric in metrics:\n",
        "    plt.barh(df[\"Model\"], df[metric], label=metric)\n",
        "\n",
        "plt.xlabel(\"Metric Values\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.title(\"Model Performance Comparison (All Metrics)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ------------------ INDIVIDUAL HORIZONTAL CHARTS ------------------\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(df[\"Model\"], df[\"Accuracy\"])\n",
        "plt.xlabel(\"Accuracy\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.title(\"Accuracy per Model\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(df[\"Model\"], df[\"Precision\"])\n",
        "plt.xlabel(\"Precision\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.title(\"Precision per Model\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(df[\"Model\"], df[\"Recall\"])\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.title(\"Recall per Model\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(df[\"Model\"], df[\"F1 Score\"])\n",
        "plt.xlabel(\"F1 Score\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.title(\"F1 Score per Model\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(df[\"Model\"], df[\"ROC-AUC\"])\n",
        "plt.xlabel(\"ROC-AUC\")\n",
        "plt.ylabel(\"Model\")\n",
        "plt.title(\"ROC-AUC per Model\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Sk_WqitHNqs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# -------------------- Prepare Data --------------------\n",
        "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"]\n",
        "model_names = df['Model'].tolist()\n",
        "metric_values = df[metrics].values\n",
        "\n",
        "# Convert metrics to angles\n",
        "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "angles += angles[:1]  # Complete the loop\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# -------------------- Radar Chart --------------------\n",
        "for i, model in enumerate(model_names):\n",
        "    values = metric_values[i].tolist()\n",
        "    values += values[:1]  # loop back\n",
        "\n",
        "    plt.polar(angles, values, marker='o', linewidth=2, label=model)\n",
        "    plt.fill(angles, values, alpha=0.1)\n",
        "\n",
        "# Labels\n",
        "plt.xticks(angles[:-1], metrics, fontsize=12)\n",
        "plt.yticks([0.2, 0.4, 0.6, 0.8, 1.0], color=\"grey\", size=10)\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "plt.title(\"Model Performance Radar Chart\", fontsize=15)\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHyzKZD-IQ5U"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# -------------------- Prepare Heatmap Data --------------------\n",
        "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"]\n",
        "heatmap_data = df.set_index('Model')[metrics]\n",
        "\n",
        "# -------------------- Create Heatmap --------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(\n",
        "    heatmap_data,\n",
        "    annot=True,\n",
        "    fmt=\".3f\",\n",
        "    cmap=\"YlGnBu\",\n",
        "    linewidths=.5,\n",
        "    cbar_kws={\"label\": \"Score\"}\n",
        ")\n",
        "\n",
        "plt.title(\"Model Evaluation Heatmap â€“ Real-Time Sentiment\", fontsize=14)\n",
        "plt.xlabel(\"Metrics\", fontsize=12)\n",
        "plt.ylabel(\"Models\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwgZ8iTEDPaF"
      },
      "source": [
        "## Mental health\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcMwuFhYDRnt"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def extract_stylometric_features(text: str) -> dict:\n",
        "    \"\"\"\n",
        "    Extract stylometric features for text forensics.\n",
        "    Features include:\n",
        "    - Sentence complexity\n",
        "    - Repetition of phrases\n",
        "    - Negations\n",
        "    - Passive voice (simplified detection)\n",
        "    - Abrupt punctuation (!!!, ...)\n",
        "    \"\"\"\n",
        "    # Sentence complexity (average words per sentence)\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    avg_words_per_sentence = sum(len(s.split()) for s in sentences) / max(len(sentences),1)\n",
        "\n",
        "    # Repetition of phrases (simple n-gram repetition)\n",
        "    words = text.split()\n",
        "    bigrams = [' '.join(words[i:i+2]) for i in range(len(words)-1)]\n",
        "    repeated_phrases = sum(1 for phrase, count in Counter(bigrams).items() if count > 1)\n",
        "\n",
        "    # Negations (common Bengali negation words)\n",
        "    negations = len(re.findall(r'à¦¨à¦¾|à¦¨à¦‡|à¦¨', text))\n",
        "\n",
        "    # Passive voice (simplified: look for \"à¦¹à¦¯à¦¼à§‡à¦›à§‡\" or \"à¦¹à¦“à¦¯à¦¼à¦¾\" patterns)\n",
        "    passive_markers = len(re.findall(r'à¦¹à¦¯à¦¼à§‡à¦›à§‡|à¦¹à¦“à¦¯à¦¼à¦¾', text))\n",
        "\n",
        "    # Abrupt punctuation\n",
        "    abrupt_punct = len(re.findall(r'(\\!\\!\\!|\\.\\.\\.)', text))\n",
        "\n",
        "    features = {\n",
        "        \"avg_words_per_sentence\": avg_words_per_sentence,\n",
        "        \"repeated_phrases\": repeated_phrases,\n",
        "        \"negations_count\": negations,\n",
        "        \"passive_voice_count\": passive_markers,\n",
        "        \"abrupt_punctuation_count\": abrupt_punct\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "features = extract_stylometric_features(user_text)\n",
        "print(\"Stylometric Features:\", features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xjMzxIEovz2"
      },
      "outputs": [],
      "source": [
        "f=final_emotion\n",
        "f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSgawBtOFo1B"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Dummy session messages\n",
        "session_messages = [\n",
        "    {\"timestamp\": datetime.now() - timedelta(minutes=30), \"emotion\": \"joy\"},\n",
        "    {\"timestamp\": datetime.now() - timedelta(minutes=20), \"emotion\": \"anxiety\"},\n",
        "    {\"timestamp\": datetime.now() - timedelta(minutes=10), \"emotion\": \"sadness\"},\n",
        "\n",
        "    {\"timestamp\": datetime.now() - timedelta(minutes=10), \"emotion\": \"suicidial\"},\n",
        "    {\"timestamp\": datetime.now(), \"emotion\": \"neutral\"},\n",
        "\n",
        "    {\"timestamp\": datetime.now()  - timedelta(minutes=10), \"emotion\": f}\n",
        "\n",
        "]\n",
        "\n",
        "# Base emotion map\n",
        "emotion_map = {\"joy\": 6, \"neutral\": 4, \"anxiety\": 3, \"sadness\": 2,\"anger\":5,\"suicidial\":1}\n",
        "\n",
        "# Prepare data\n",
        "times = [msg[\"timestamp\"] for msg in session_messages]\n",
        "emotion_values = [emotion_map.get(msg[\"emotion\"], 3) for msg in session_messages]\n",
        "\n",
        "# Set colors: highlight the final emotion in red\n",
        "colors = [\"blue\"] * (len(session_messages) - 1) + [\"blue\"]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.scatter(times, emotion_values, color=colors, s=100, zorder=5)  # Points\n",
        "plt.plot(times, emotion_values, linestyle='-', color='blue', alpha=0.5, zorder=1)  # Line\n",
        "plt.yticks(list(emotion_map.values()), list(emotion_map.keys()))\n",
        "plt.title(\"Emotion Shift Timeline (Final Emotion Highlighted)\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Emotion\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRMkMECcGHDB"
      },
      "outputs": [],
      "source": [
        "def calculate_risk_score(emotion_list: list[str], stylometric_features: dict) -> str:\n",
        "    \"\"\"\n",
        "    Assign a risk level based on:\n",
        "    - Repeated negative emotions (sadness, anxiety, anger, suicidal hints)\n",
        "    - Stylometric warning signs (negations, abrupt punctuation, passive voice)\n",
        "    \"\"\"\n",
        "\n",
        "    # Correct negative emotions list\n",
        "    negative_emotions = [\"anxiety\", \"sadness\", \"anger\", \"suicidal\"]\n",
        "\n",
        "    # Count negative emotions\n",
        "    negative_count = sum(1 for e in emotion_list if e in negative_emotions)\n",
        "\n",
        "    # Compute risk score (weighted sum)\n",
        "    score = negative_count * 2\n",
        "    score += stylometric_features.get(\"negations_count\", 0)\n",
        "    score += stylometric_features.get(\"abrupt_punctuation_count\", 0)\n",
        "    score += stylometric_features.get(\"passive_voice_count\", 0)\n",
        "\n",
        "    # Assign risk category\n",
        "    if score >= 7:\n",
        "        risk = \"High emotional risk\"\n",
        "    elif score >= 4:\n",
        "        risk = \"Medium emotional risk\"\n",
        "    else:\n",
        "        risk = \"Low emotional risk\"\n",
        "\n",
        "    return risk\n",
        "\n",
        "# -------------------------------\n",
        "# Example usage\n",
        "# -------------------------------\n",
        "# If your text is \"i am khub valo\" â†’ positive emotion\n",
        "emotion_history = f  # Correctly classified\n",
        "\n",
        "# Example stylometric features (neutral)\n",
        "features = {\n",
        "    \"negations_count\": 0,\n",
        "    \"abrupt_punctuation_count\": 0,\n",
        "    \"passive_voice_count\": 0\n",
        "}\n",
        "\n",
        "risk = calculate_risk_score(emotion_history, features)\n",
        "print(\"Risk Level:\", risk)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj3ibu_YGL72"
      },
      "outputs": [],
      "source": [
        "def smart_alert_system(user_id: str, risk_level: str):\n",
        "    \"\"\"\n",
        "    Trigger alerts or gentle notifications based on risk level.\n",
        "    - High risk: send helpline links / backend monitoring alert\n",
        "    - Medium/Low: optional gentle tips\n",
        "    \"\"\"\n",
        "    resources = {\n",
        "        \"helpline\": \"108 / www.mentalhealthbn.org\",\n",
        "        \"therapy_links\": [\"https://therapybn.example.com\"]\n",
        "    }\n",
        "\n",
        "    if risk_level == \"High emotional risk\":\n",
        "        alert_msg = f\"âš ï¸ {user_id}, your emotional risk is high. Consider contacting: {resources['helpline']}\"\n",
        "        backend_alert = True\n",
        "    elif risk_level == \"Medium emotional risk\":\n",
        "        alert_msg = f\"{user_id}, your mood seems low. Try visiting: {resources['therapy_links'][0]}\"\n",
        "        backend_alert = False\n",
        "    else:\n",
        "        alert_msg = f\"{user_id}, your mood is stable.\"\n",
        "        backend_alert = False\n",
        "\n",
        "    return {\"alert_message\": alert_msg, \"backend_alert_triggered\": backend_alert}\n",
        "\n",
        "# Example usage\n",
        "alert = smart_alert_system(\"user_demo\", risk)\n",
        "print(alert)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJYVDcyFGqbb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def emotion_trend_dashboard(user_emotions: list[dict]):\n",
        "    \"\"\"\n",
        "    Display trends:\n",
        "    - Most frequent emotion types\n",
        "    - Time-based variation\n",
        "    - Risk spikes\n",
        "    \"\"\"\n",
        "    # Count frequency\n",
        "    all_emotions = [entry[\"emotion\"] for entry in user_emotions]\n",
        "    freq = Counter(all_emotions)\n",
        "\n",
        "    # Plot bar chart of frequency\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.bar(freq.keys(), freq.values(), color='skyblue')\n",
        "    plt.title(\"Most Frequent Emotions\")\n",
        "    plt.show()\n",
        "\n",
        "    # Time-based variation (line graph)\n",
        "    times = [entry[\"timestamp\"] for entry in user_emotions]\n",
        "    emotion_map = {\"joy\":4, \"neutral\":3, \"anxiety\":2, \"sadness\":1, \"anger\":1}\n",
        "    values = [emotion_map.get(e,3) for e in all_emotions]\n",
        "    plt.figure(figsize=(8,3))\n",
        "    plt.plot(times, values, marker='o', linestyle='-', color='orange')\n",
        "    plt.title(\"Emotion Over Time\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Emotion Level\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return freq\n",
        "\n",
        "# Example usage\n",
        "user_emotions = [\n",
        "    {\"timestamp\": 1, \"emotion\":\"joy\"},\n",
        "    {\"timestamp\": 2, \"emotion\":\"sadness\"},\n",
        "    {\"timestamp\": 3, \"emotion\":\"joy\"},\n",
        "    {\"timestamp\": 4, \"emotion\":\"anxiety\"},\n",
        "    {\"timestamp\": 5, \"emotion\": f},\n",
        "]\n",
        "emotion_trend_dashboard(user_emotions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f4w_1lbGuw-"
      },
      "outputs": [],
      "source": [
        "def depression_marker(texts: list[str], user_baseline: dict):\n",
        "    \"\"\"\n",
        "    Compare current writing with user baseline.\n",
        "    Detect early depression markers:\n",
        "    - Shorter sentences\n",
        "    - Reduced vocabulary variety\n",
        "    - Increased negative words\n",
        "    \"\"\"\n",
        "    negative_words = [\"à¦¦à§à¦ƒà¦–\", \"à¦¬à¦¿à¦°à¦•à§à¦¤à¦¿\", \"à¦•à¦·à§à¦Ÿ\", \"à¦­à¦¯à¦¼\"]\n",
        "\n",
        "    total_sentences = sum(len(t.split('.')) for t in texts)\n",
        "    total_words = sum(len(t.split()) for t in texts)\n",
        "    avg_words_per_sentence = total_words / max(total_sentences,1)\n",
        "\n",
        "    vocab_set = set(word for t in texts for word in t.split())\n",
        "    vocab_size = len(vocab_set)\n",
        "\n",
        "    negative_count = sum(sum(1 for w in t.split() if w in negative_words) for t in texts)\n",
        "\n",
        "    risk_flag = (avg_words_per_sentence < user_baseline.get(\"avg_words_per_sentence\",12)\n",
        "                or vocab_size < user_baseline.get(\"vocab_size\",50)\n",
        "                or negative_count > user_baseline.get(\"neg_word_count\",2))\n",
        "\n",
        "    return {\"avg_words_per_sentence\": avg_words_per_sentence,\n",
        "            \"vocab_size\": vocab_size,\n",
        "            \"negative_word_count\": negative_count,\n",
        "            \"possible_depression_marker\": risk_flag}\n",
        "\n",
        "# Example usage\n",
        "baseline = {\"avg_words_per_sentence\":12, \"vocab_size\":50, \"neg_word_count\":2}\n",
        "texts = user_text\n",
        "marker = depression_marker(texts, baseline)\n",
        "print(marker)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Macqg8qGyey"
      },
      "outputs": [],
      "source": [
        "def detect_polarity_intensity(text: str) -> dict:\n",
        "    \"\"\"\n",
        "    Determine emotion intensity:\n",
        "    - Low, Moderate, High\n",
        "    Simple rule-based: exclamation marks or strong adjectives\n",
        "    \"\"\"\n",
        "    intensity = \"low\"\n",
        "    if \"à¦…à¦¸à¦¾à¦§à¦¾à¦°à¦£\" in text or \"!!!\" in text or \"à¦–à§à¦¬\" in text:\n",
        "        intensity = \"high\"\n",
        "    elif \"à¦­à¦¾à¦²à§‹\" in text or \"à¦ à¦¿à¦•\" in text:\n",
        "        intensity = \"moderate\"\n",
        "\n",
        "    return {\"text\": text, \"intensity\": intensity}\n",
        "\n",
        "# Example usage\n",
        "print(detect_polarity_intensity(\"à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à¦›à§‡\"))\n",
        "print(detect_polarity_intensity(\"à¦…à¦¸à¦¾à¦§à¦¾à¦°à¦£ à¦²à¦¾à¦—à¦›à§‡!!!\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbqfES36G1AL"
      },
      "outputs": [],
      "source": [
        "def longitudinal_mood_graph(messages: list[dict], period: str=\"day\"):\n",
        "    \"\"\"\n",
        "    Visualize mood history over days/weeks\n",
        "    messages: list of {\"timestamp\": datetime, \"emotion\": str}\n",
        "    \"\"\"\n",
        "    import matplotlib.dates as mdates\n",
        "\n",
        "    emotion_map = {\"joy\":4, \"neutral\":3, \"anxiety\":2, \"sadness\":1, \"anger\":1}\n",
        "    times = [msg[\"timestamp\"] for msg in messages]\n",
        "    values = [emotion_map.get(msg[\"emotion\"],3) for msg in messages]\n",
        "\n",
        "    plt.figure(figsize=(8,3))\n",
        "    plt.plot(times, values, marker='o', color='green')\n",
        "    plt.title(\"Longitudinal Mood Graph\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Mood Level\")\n",
        "\n",
        "    if period == \"week\":\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%W-%Y'))\n",
        "    else:\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m %H:%M'))\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "from datetime import datetime, timedelta\n",
        "messages = [{\"timestamp\": datetime.now()-timedelta(days=i), \"emotion\":random.choice([\"joy\",\"sadness\",\"anxiety\",\"neutral\"])} for i in range(7)]\n",
        "longitudinal_mood_graph(messages, period=\"day\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCk9UrRkG3c1"
      },
      "outputs": [],
      "source": [
        "def therapy_feedback_assistant(emotion: str, intensity: str) -> dict:\n",
        "    \"\"\"\n",
        "    Provide emotion-specific mental health support suggestions:\n",
        "    - Coping message\n",
        "    - Breathing exercise\n",
        "    - Journaling prompt\n",
        "    \"\"\"\n",
        "\n",
        "    emotion_support = {\n",
        "        \"sad\": {\n",
        "            \"coping\": \"à¦®à¦¨ à¦–à¦¾à¦°à¦¾à¦ª à¦¹à¦²à§‡ à¦¨à¦¿à¦œà§‡à¦° à¦ªà§à¦°à¦¤à¦¿ à¦à¦•à¦Ÿà§ à¦•à§‹à¦®à¦² à¦¹à¦¨à¥¤ à¦§à§€à¦°à§‡ à¦§à§€à¦°à§‡ à¦¨à¦¿à¦œà§‡à¦•à§‡ à¦¸à¦¾à¦®à¦²à¦¾à¦¨à¥¤\",\n",
        "            \"breathing\": \"à§ª à¦¸à§‡à¦•à§‡à¦¨à§à¦¡ à¦¶à§à¦¬à¦¾à¦¸ à¦¨à¦¿à¦¨, à§¬ à¦¸à§‡à¦•à§‡à¦¨à§à¦¡à§‡ à¦›à¦¾à¦¡à¦¼à§à¦¨ â€” à¦à¦Ÿà¦¿ à¦¦à§à¦ƒà¦– à¦•à¦®à¦¾à¦¤à§‡ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯ à¦•à¦°à§‡à¥¤\",\n",
        "            \"journal\": \"à¦†à¦œ à¦•à§‹à¦¨ à¦˜à¦Ÿà¦¨à¦¾ à¦†à¦ªà¦¨à¦¾à¦° à¦®à¦¨ à¦–à¦¾à¦°à¦¾à¦ªà§‡à¦° à¦•à¦¾à¦°à¦£ à¦¹à¦²à§‹? à¦†à¦ªà¦¨à¦¿ à¦•à§€à¦­à¦¾à¦¬à§‡ à¦¨à¦¿à¦œà§‡à¦•à§‡ à¦¸à¦¹à¦¾à¦¯à¦¼à¦¤à¦¾ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¦¨?\"\n",
        "        },\n",
        "        \"happy\": {\n",
        "            \"coping\": \"à¦à¦‡ à¦†à¦¨à¦¨à§à¦¦à¦Ÿà¦¾ à¦§à¦°à§‡ à¦°à¦¾à¦–à§à¦¨! à¦›à§‹à¦Ÿ à¦›à§‹à¦Ÿ à¦œà¦¿à¦¨à¦¿à¦¸ à¦‰à¦ªà¦­à§‹à¦— à¦•à¦°à§à¦¨à¥¤\",\n",
        "            \"breathing\": \"à¦§à§€à¦°à§‡ à¦§à§€à¦°à§‡ à¦—à¦­à§€à¦° à¦¶à§à¦¬à¦¾à¦¸ à¦¨à¦¿à¦¨ à¦à¦¬à¦‚ à¦®à§à¦¹à§‚à¦°à§à¦¤à¦Ÿà¦¿à¦•à§‡ à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à§à¦¨à¥¤\",\n",
        "            \"journal\": \"à¦†à¦œ à¦•à§€ à¦†à¦ªà¦¨à¦¾à¦•à§‡ à¦¸à¦¬à¦šà§‡à¦¯à¦¼à§‡ à¦–à§à¦¶à¦¿ à¦•à¦°à§‡à¦›à§‡? à¦­à¦¬à¦¿à¦·à§à¦¯à¦¤à§‡ à¦à¦‡ à¦…à¦¨à§à¦­à§‚à¦¤à¦¿ à¦¬à¦œà¦¾à¦¯à¦¼ à¦°à¦¾à¦–à¦¾à¦° à¦‰à¦ªà¦¾à¦¯à¦¼ à¦•à§€?\"\n",
        "        },\n",
        "        \"angry\": {\n",
        "            \"coping\": \"à¦°à¦¾à¦— à¦¹à¦²à§‡ à¦¥à¦¾à¦®à§à¦¨à¥¤ à¦ªà¦°à¦¿à¦¸à§à¦¥à¦¿à¦¤à¦¿ à¦¥à§‡à¦•à§‡ à§©à§¦ à¦¸à§‡à¦•à§‡à¦¨à§à¦¡ à¦¦à§‚à¦°à§‡ à¦¥à¦¾à¦•à§à¦¨à¥¤\",\n",
        "            \"breathing\": \"à§¬ à¦¸à§‡à¦•à§‡à¦¨à§à¦¡à§‡ à¦¶à§à¦¬à¦¾à¦¸ à¦¨à¦¿à¦¨, à§¬ à¦¸à§‡à¦•à§‡à¦¨à§à¦¡ à¦§à¦°à§‡ à¦°à¦¾à¦–à§à¦¨, à§¬ à¦¸à§‡à¦•à§‡à¦¨à§à¦¡à§‡ à¦›à¦¾à¦¡à¦¼à§à¦¨à¥¤\",\n",
        "            \"journal\": \"à¦†à¦ªà¦¨à¦¾à¦•à§‡ à¦ à¦¿à¦• à¦•à§‹à¦¨ à¦¬à¦¿à¦·à¦¯à¦¼à¦Ÿà¦¿ à¦Ÿà§à¦°à¦¿à¦—à¦¾à¦° à¦•à¦°à¦²? à¦­à¦¬à¦¿à¦·à§à¦¯à¦¤à§‡ à¦†à¦ªà¦¨à¦¿ à¦•à§€à¦­à¦¾à¦¬à§‡ à¦­à¦¿à¦¨à§à¦¨à¦­à¦¾à¦¬à§‡ à¦ªà§à¦°à¦¤à¦¿à¦•à§à¦°à¦¿à¦¯à¦¼à¦¾ à¦œà¦¾à¦¨à¦¾à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¦¨?\"\n",
        "        },\n",
        "        \"anxiety\": {\n",
        "            \"coping\": \"à¦­à¦¯à¦¼ à¦¸à§à¦¬à¦¾à¦­à¦¾à¦¬à¦¿à¦•à¥¤ à¦§à§€à¦°à§‡ à¦§à§€à¦°à§‡ à¦¨à¦¿à¦œà§‡à¦° à¦¨à¦¿à¦°à¦¾à¦ªà¦¤à§à¦¤à¦¾ à¦®à¦¨à§‡ à¦•à¦°à¦¿à¦¯à¦¼à§‡ à¦¦à¦¿à¦¨à¥¤\",\n",
        "            \"breathing\": \"à§«-à§«-à§« à¦¶à§à¦¬à¦¾à¦¸ à¦ªà¦¦à§à¦§à¦¤à¦¿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§à¦¨: à§« à¦¸à§‡à¦•à§‡à¦¨à§à¦¡ à¦¶à§à¦¬à¦¾à¦¸ à¦¨à¦¿à¦¨, à§« à¦§à¦°à§‡ à¦°à¦¾à¦–à§à¦¨, à§« à¦¸à§‡à¦•à§‡à¦¨à§à¦¡ à¦›à¦¾à¦¡à¦¼à§à¦¨à¥¤\",\n",
        "            \"journal\": \"à¦†à¦ªà¦¨à¦¾à¦° à¦­à¦¯à¦¼à§‡à¦° à¦‰à§Žà¦¸ à¦•à§€? à¦à¦Ÿà¦¿ à¦†à¦¸à¦²à§‡à¦‡ à¦•à¦¤à¦Ÿà¦¾ à¦¬à¦¿à¦ªà¦œà§à¦œà¦¨à¦•?\"\n",
        "        },\n",
        "        \"surprise\": {\n",
        "            \"coping\": \"à¦šà¦®à¦• à¦•à¦–à¦¨à¦“ à¦­à¦¾à¦²à§‹, à¦•à¦–à¦¨à¦“ à¦¬à¦¿à¦­à§à¦°à¦¾à¦¨à§à¦¤à¦¿à¦•à¦°à¥¤ à¦ªà§à¦°à¦¥à¦®à§‡ à¦¬à¦¿à¦·à¦¯à¦¼à¦Ÿà¦¿ à¦¬à§‹à¦à¦¾à¦° à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à§à¦¨à¥¤\",\n",
        "            \"breathing\": \"à§©à¦Ÿà¦¿ à¦—à¦­à§€à¦° à¦¶à§à¦¬à¦¾à¦¸ à¦¨à¦¿à¦¨ à¦à¦¬à¦‚ à¦ªà¦°à¦¿à¦¸à§à¦¥à¦¿à¦¤à¦¿ à¦¨à¦¿à¦°à§€à¦•à§à¦·à¦¾ à¦•à¦°à§à¦¨à¥¤\",\n",
        "            \"journal\": \"à¦šà¦®à¦•à§‡à¦° à¦•à¦¾à¦°à¦£ à¦•à§€ à¦›à¦¿à¦² à¦à¦¬à¦‚ à¦†à¦ªà¦¨à¦¿ à¦¤à¦¾ à¦•à§€à¦­à¦¾à¦¬à§‡ à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à§‡à¦›à§‡à¦¨?\"\n",
        "        },\n",
        "        \"disgust\": {\n",
        "            \"coping\": \"à¦à¦‡ à¦…à¦¨à§à¦­à§‚à¦¤à¦¿ à¦†à¦ªà¦¨à¦¾à¦•à§‡ à¦¸à§€à¦®à¦¾à¦¨à¦¾ à¦°à¦•à§à¦·à¦¾ à¦•à¦°à¦¤à§‡ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯ à¦•à¦°à§‡à¥¤\",\n",
        "            \"breathing\": \"à§ª-à§ª à¦¶à§à¦¬à¦¾à¦¸ (à§ª à¦¶à§à¦¬à¦¾à¦¸, à§ª à¦›à¦¾à¦¡à¦¼à¦¾) à¦®à¦¨ à¦¶à¦¾à¦¨à§à¦¤ à¦•à¦°à§‡à¥¤\",\n",
        "            \"journal\": \"à¦†à¦ªà¦¨à¦¿ à¦•à§€ à¦¦à§‡à¦–à§‡ à¦¬à¦¾ à¦¶à§à¦¨à§‡ à¦…à¦¸à§à¦¬à¦¸à§à¦¤à¦¿ à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à§‡à¦›à§‡à¦¨? à¦à¦Ÿà¦¿ à¦†à¦ªà¦¨à¦¾à¦° à¦®à§‚à¦²à§à¦¯à¦¬à§‹à¦§à§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦•à§€à¦­à¦¾à¦¬à§‡ à¦¯à§à¦•à§à¦¤?\"\n",
        "        },\n",
        "        \"neutral\": {\n",
        "            \"coping\": \"à¦†à¦ªà¦¨à¦¿ à¦¸à§à¦¥à¦¿à¦° à¦†à¦›à§‡à¦¨â€”à¦à¦Ÿà¦¿ à¦à¦•à¦Ÿà¦¿ à¦­à¦¾à¦²à§‹ à¦®à§à¦¹à§‚à¦°à§à¦¤ à¦šà¦¿à¦¨à§à¦¤à¦¾ à¦ªà¦°à¦¿à¦·à§à¦•à¦¾à¦° à¦•à¦°à¦¾à¦°à¥¤\",\n",
        "            \"breathing\": \"à¦¸à§à¦¬à¦¾à¦­à¦¾à¦¬à¦¿à¦• à¦—à¦¤à¦¿à¦¤à§‡ à¦§à§€à¦°à§‡ à¦§à§€à¦°à§‡ à¦¶à§à¦¬à¦¾à¦¸ à¦¨à¦¿à¦¨à¥¤\",\n",
        "            \"journal\": \"à¦†à¦œ à¦†à¦ªà¦¨à¦¿ à¦•à§‡à¦®à¦¨ à¦†à¦›à§‡à¦¨? à¦•à§‹à¦¨à§‹ à¦¬à¦¿à¦¶à§‡à¦· à¦•à¦¿à¦›à§ à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à¦›à§‡à¦¨ à¦•à¦¿?\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Default fallback if emotion is unknown\n",
        "    if emotion not in emotion_support:\n",
        "        return {\n",
        "            \"coping_message\": \"à¦¨à¦¿à¦œà§‡à¦•à§‡ à¦à¦•à¦Ÿà§ à¦¸à¦®à¦¯à¦¼ à¦¦à¦¿à¦¨à¥¤ à¦…à¦¨à§à¦­à§‚à¦¤à¦¿à¦—à§à¦²à§‹ à¦¸à§à¦¬à¦¾à¦­à¦¾à¦¬à¦¿à¦•à¥¤\",\n",
        "            \"breathing_exercise\": \"à¦—à¦­à§€à¦° à¦¶à§à¦¬à¦¾à¦¸ à¦¨à¦¿à¦¨ à§«-à¦¸à§‡à¦•à§‡à¦¨à§à¦¡ à¦ªà§à¦¯à¦¾à¦Ÿà¦¾à¦°à§à¦¨à§‡à¥¤\",\n",
        "            \"journaling_prompt\": f\"à¦†à¦ªà¦¨à¦¿ à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨à§‡ à¦•à§€ à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à¦›à§‡à¦¨? à¦®à¦¾à¦¤à§à¦°à¦¾: {intensity}\"\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"coping_message\": emotion_support[emotion][\"coping\"],\n",
        "        \"breathing_exercise\": emotion_support[emotion][\"breathing\"],\n",
        "        \"journaling_prompt\": f\"{emotion_support[emotion]['journal']} (Intensity: {intensity})\"\n",
        "    }\n",
        "print(therapy_feedback_assistant(f, risk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQuqMdiwofqW"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Follow-up question generator (Bengali)\n",
        "# ---------------------------\n",
        "def generate_followup_question(emotion: str) -> str:\n",
        "    \"\"\"\n",
        "    Return a personalized follow-up question in Bengali for the given emotion.\n",
        "    \"\"\"\n",
        "    q_map = {\n",
        "        \"anxiety\": \"à¦†à¦®à¦¿ à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à¦›à¦¿ à¦†à¦ªà¦¨à¦¿ à¦†à¦œ anxiety à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à¦›à§‡à¦¨à¥¤ à¦à¦Ÿà¦¾ à¦•à¦¿ à¦•à¦¾à¦œ, à¦¸à¦®à§à¦ªà¦°à§à¦• à¦¨à¦¾à¦•à¦¿ à¦¨à¦¿à¦œà§‡à¦° à¦•à§‹à¦¨à§‹ à¦­à¦¾à¦¬à¦¨à¦¾à¦° à¦•à¦¾à¦°à¦£à§‡ à¦¹à¦¯à¦¼à§‡à¦›à§‡?\",\n",
        "        \"sadness\": \"à¦†à¦ªà¦¨à¦¿ à¦¦à§à¦ƒà¦– à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à§‡à¦›à§‡à¦¨ â€” à¦•à¦¿ à¦˜à¦Ÿà¦¨à¦¾ à¦¬à¦¾ à¦šà¦¿à¦¨à§à¦¤à¦¾ à¦à¦Ÿà¦¾ à¦Ÿà§à¦°à¦¿à¦—à¦¾à¦° à¦•à¦°à§‡à¦›à§‡?\",\n",
        "        \"joy\": \"à¦–à§à¦¬ à¦­à¦¾à¦² à¦²à¦¾à¦—à¦›à§‡! à¦•à¦¿ à¦˜à¦Ÿà§‡à¦›à§‡ à¦¯à¦¾ à¦†à¦ªà¦¨à¦¾à¦•à§‡ à¦à¦¤ à¦–à§à¦¶à¦¿ à¦•à¦°à§‡à¦›à§‡?\",\n",
        "        \"anger\": \"à¦†à¦®à¦¿ à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à¦›à¦¿ à¦†à¦ªà¦¨à¦¿ à¦°à§‡à¦—à§‡ à¦†à¦›à§‡à¦¨ â€” à¦à¦Ÿà¦¾à¦° à¦•à¦¾à¦°à¦£ à¦•à¦¿ à¦¬à¦¾ à¦•à¦¿ à¦Ÿà§à¦°à¦¿à¦—à¦¾à¦° à¦•à¦°à¦²?\",\n",
        "        \"neutral\": \"à¦ à¦¿à¦• à¦†à¦›à§‡à¦¨ à¦¤à§‹? à¦•à¦¿ à¦¨à¦¿à¦¯à¦¼à§‡ à¦†à¦œ à¦†à¦ªà¦¨à¦¾à¦° à¦®à¦¨ à¦¸à§à¦¥à¦¿à¦° à¦†à¦›à§‡?\",\n",
        "        \"surprise\": \"à¦à¦Ÿà¦¾ à¦•à¦¿ à¦‡à¦¤à¦¿à¦¬à¦¾à¦šà¦• à¦¨à¦¾à¦•à¦¿ à¦‰à¦¦à§à¦¬à§‡à¦—à¦œà¦¨à¦• à¦šà¦®à¦• à¦›à¦¿à¦²?\",\n",
        "        \"disgust\": \"à¦•à§€ à¦¦à§‡à¦–à¦²à§‡ à¦¬à¦¾ à¦¶à§à¦¨à¦²à§‡ à¦…à¦¸à§à¦¬à¦¸à§à¦¤à¦¿ à¦²à¦¾à¦—à¦²? à¦†à¦ªà¦¨à¦¿ à¦•à¦¿ à¦¨à¦¿à¦°à¦¾à¦ªà¦¦ à¦¬à§‹à¦§ à¦•à¦°à¦›à§‡à¦¨?\",\n",
        "        \"suicidal\": \"à¦†à¦®à¦¿ à¦šà¦¿à¦¨à§à¦¤à¦¿à¦¤ â€” à¦†à¦ªà¦¨à¦¿ à¦•à¦¿ à¦à¦–à¦¨ à¦¨à¦¿à¦°à¦¾à¦ªà¦¦? à¦†à¦ªà¦¨à¦¿ à¦•à¦¾à¦‰à¦•à§‡ à¦ªà¦¾à¦¶à§‡ à¦ªà§‡à§Ÿà§‡à¦›à§‡à¦¨ à¦•à¦¾à¦° à¦¸à¦¾à¦¥à§‡ à¦•à¦¥à¦¾ à¦¬à¦²à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¦¨?\"\n",
        "    }\n",
        "    return q_map.get(emotion.lower(), \"à¦†à¦ªà¦¨à¦¿ à¦•à§‡à¦®à¦¨ à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à¦›à§‡à¦¨? à¦†à¦ªà¦¨à¦¿ à¦•à¦¿ à¦à¦•à¦Ÿà§ à¦¬à¦¿à¦¶à¦¦ à¦¬à¦²à¦¬à§‡à¦¨?\")\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Mood history storage helpers + insights\n",
        "# ---------------------------\n",
        "def append_mood(history: List[Dict], emotion: str, intensity: int, timestamp: Optional[datetime.datetime]=None) -> None:\n",
        "    \"\"\"\n",
        "    Append an emotion event to the history list.\n",
        "    Each event: {\"timestamp\": datetime, \"emotion\": str, \"intensity\": int}\n",
        "    intensity: e.g., 1-10\n",
        "    \"\"\"\n",
        "    if timestamp is None:\n",
        "        timestamp = datetime.datetime.now()\n",
        "    history.append({\"timestamp\": timestamp, \"emotion\": emotion.lower(), \"intensity\": int(intensity)})\n",
        "\n",
        "def get_last_n_days(history: List[Dict], days: int=7) -> List[Dict]:\n",
        "    cutoff = datetime.datetime.now() - datetime.timedelta(days=days)\n",
        "    return [e for e in history if e[\"timestamp\"] >= cutoff]\n",
        "\n",
        "def generate_weekly_insights(history: List[Dict]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate simple human-readable insights from last 7 days.\n",
        "    \"\"\"\n",
        "    last_week = get_last_n_days(history, days=7)\n",
        "    if not last_week:\n",
        "        return [\"No mood data for the last 7 days.\"]\n",
        "\n",
        "    # Frequency\n",
        "    freq = Counter(e[\"emotion\"] for e in last_week)\n",
        "    most_common = freq.most_common(1)[0][0]\n",
        "\n",
        "    # Average intensity by hour-of-day to detect peak times\n",
        "    by_hour = defaultdict(list)\n",
        "    for e in last_week:\n",
        "        h = e[\"timestamp\"].hour\n",
        "        by_hour[h].append(e[\"intensity\"])\n",
        "    avg_by_hour = {h: sum(vals)/len(vals) for h, vals in by_hour.items()}\n",
        "    if avg_by_hour:\n",
        "        peak_hour = max(avg_by_hour, key=avg_by_hour.get)\n",
        "    else:\n",
        "        peak_hour = None\n",
        "\n",
        "    insights = []\n",
        "    insights.append(f\"Most seen emotion: {most_common} ({freq[most_common]} à¦¬à¦¾à¦°)à¥¤\")\n",
        "    if peak_hour is not None:\n",
        "        insights.append(f\"à¦—à¦¡à¦¼ à¦‰à¦¦à§à¦¬à§‡à¦—/à¦®à§à¦¡ à¦¸à§‡à¦°à¦¾ à¦¸à¦®à¦¯à¦¼: à¦ªà§à¦°à¦¾à¦¯à¦¼ {peak_hour}:00 - à¦¤à¦¾à¦°à¦ªà¦°à§‡ à¦°à¦¾à¦¤à¦­à§‹à¦°/à¦¸à¦¨à§à¦§à§à¦¯à¦¾ à¦ªà¦°à§à¦¯à¦¾à§Ÿà§‡ à¦¤à§à¦²à¦¨à¦¾ à¦•à¦°à§à¦¨à¥¤\")\n",
        "    # compare this week vs previous week (simple)\n",
        "    cutoff = datetime.datetime.now() - datetime.timedelta(days=14)\n",
        "    prev_window = [e for e in history if cutoff <= e[\"timestamp\"] < datetime.datetime.now()-datetime.timedelta(days=7)]\n",
        "    if prev_window:\n",
        "        prev_freq = Counter(e[\"emotion\"] for e in prev_window)\n",
        "        # find if negative emotions increased\n",
        "        negative = {\"anxiety\",\"sadness\",\"suicidal\",\"anger\"}\n",
        "        cur_neg = sum(v for k,v in freq.items() if k in negative)\n",
        "        prev_neg = sum(v for k,v in prev_freq.items() if k in negative)\n",
        "        if cur_neg > prev_neg:\n",
        "            insights.append(\"à¦¨à¦¿à¦¯à¦¼à¦®à¦¿à¦¤ à¦¨à§‡à¦—à§‡à¦Ÿà¦¿à¦­ à¦…à¦¨à§à¦­à§‚à¦¤à¦¿à¦° à¦¸à¦‚à¦–à§à¦¯à¦¾ à¦¬à¦¾à¦¡à¦¼à¦›à§‡ â€” à¦¨à¦œà¦° à¦°à¦¾à¦–à§à¦¨à¥¤\")\n",
        "        elif cur_neg < prev_neg:\n",
        "            insights.append(\"à¦†à¦ªà¦¨à¦¾à¦° à¦®à§à¦¡à§‡ à¦‰à¦¨à§à¦¨à¦¤à¦¿ à¦¹à¦¯à¦¼à§‡à¦›à§‡ â€” à¦•à¦·à§à¦Ÿà§‡à¦° à¦¦à¦¿à¦¨ à¦•à¦®à§‡à¦›à§‡à¥¤\")\n",
        "        else:\n",
        "            insights.append(\"à¦—à¦¤ à¦¦à§à¦‡ à¦¸à¦ªà§à¦¤à¦¾à¦¹à§‡à¦° à¦¤à§à¦²à¦¨à¦¾à¦¯à¦¼ à¦¨à§‡à¦—à§‡à¦Ÿà¦¿à¦­ à¦®à§à¦¡ à¦à¦•à¦‡ à¦§à¦°à§‡ à¦†à¦›à§‡à¥¤\")\n",
        "    return insights\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Mood trend plotting (matplotlib)\n",
        "# ---------------------------\n",
        "def plot_mood_trend(history: List[Dict], period: str=\"7d\"):\n",
        "    \"\"\"\n",
        "    Plot mood over time. history entries must contain timestamp and emotion or intensity.\n",
        "    Uses matplotlib. For emotions without numeric intensity, maps simple values.\n",
        "    \"\"\"\n",
        "    if not history:\n",
        "        print(\"No history to plot.\")\n",
        "        return\n",
        "\n",
        "    # Map emotion to numeric if intensity not present\n",
        "    emotion_base_map = {\"joy\":5, \"happy\":5, \"neutral\":3, \"anxiety\":2, \"sadness\":1, \"anger\":1, \"surprise\":4, \"disgust\":1, \"suicidal\":0}\n",
        "    times = [e[\"timestamp\"] for e in history]\n",
        "    values = [e.get(\"intensity\", emotion_base_map.get(e[\"emotion\"], 3)) for e in history]\n",
        "\n",
        "    plt.figure(figsize=(9,3))\n",
        "    plt.plot(times, values, marker='o', linestyle='-', linewidth=1, zorder=2)\n",
        "    plt.scatter(times, values, s=50, zorder=3)\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m %H:%M'))\n",
        "    plt.title(\"Mood Trend\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Intensity / Mood Level\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Music recommendation\n",
        "# ---------------------------\n",
        "def recommend_music(emotion: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Simple emotion -> music playlist suggestions (URLs are placeholders).\n",
        "    \"\"\"\n",
        "    m = emotion.lower()\n",
        "    rec = {\n",
        "        \"sadness\": [\"Soft Instrumental Playlist - https://example.com/soft-inst\"],\n",
        "        \"anxiety\": [\"Binaural Beats 432Hz - https://example.com/binaural-432\", \"Calming breath music - https://example.com/breath\"],\n",
        "        \"anger\": [\"Calming Ambient - https://example.com/ambient-calm\", \"Slow classical - https://example.com/classic-slow\"],\n",
        "        \"joy\": [\"Energetic playlist - https://example.com/energetic\", \"Upbeat pop - https://example.com/upbeat\"],\n",
        "        \"neutral\": [\"Light acoustic - https://example.com/acoustic\"],\n",
        "        \"surprise\": [\"Soothing reflective - https://example.com/reflect\"],\n",
        "        \"disgust\": [\"Grounding ambient - https://example.com/ground\"],\n",
        "        \"suicidal\": [\"Crisis resources (please reach out) - https://example.com/crisis\"]\n",
        "    }\n",
        "    return {\"music\": rec.get(m, [\"Calming playlist - https://example.com/calming\"])}\n",
        "\n",
        "# ---------------------------\n",
        "# 5) CBT / Cognitive reframing\n",
        "# ---------------------------\n",
        "CBT_EXAMPLES = {\n",
        "    \"sadness\": {\n",
        "        \"automatic\": \"à¦†à¦®à¦¿ à¦¸à¦¬à¦¸à¦®à§Ÿ à¦¬à§à¦¯à¦°à§à¦¥ à¦¹à¦šà§à¦›à¦¿à¥¤\",\n",
        "        \"distortion\": \"à¦…à¦²-à¦…à¦°-à¦¨à¦¾à¦¥à¦¿à¦‚ à¦¥à¦¿à¦‚à¦•à¦¿à¦‚ (à¦¸à¦¬ à¦¨à¦¾ à¦¹à¦²à§‡ à¦¬à§à¦¯à¦°à§à¦¥)\",\n",
        "        \"reframed\": \"à¦à¦•à¦Ÿà¦¾ à¦–à¦¾à¦°à¦¾à¦ª à¦¦à¦¿à¦¨ à¦®à¦¾à¦¨à§‡ à¦†à¦®à¦¿ à¦¸à¦¬à¦¸à¦®à§Ÿ à¦¬à§à¦¯à¦°à§à¦¥â€”à¦†à¦®à¦¿ à¦†à¦—à§‡ à¦­à¦¾à¦²à§‹ à¦•à¦¿à¦›à§ à¦•à¦°à¦¤à¦¾à¦® à¦à¦¬à¦‚ à¦•à¦°à¦¬à¥¤\"\n",
        "    },\n",
        "    \"anxiety\": {\n",
        "        \"automatic\": \"à¦¯à¦¦à¦¿ à¦à¦Ÿà¦¾ à¦–à¦¾à¦°à¦¾à¦ªà¦­à¦¾à¦¬à§‡ à¦¹à§Ÿ? à¦†à¦®à¦¿ à¦¤à¦¾ à¦¸à¦¹à§à¦¯ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¬ à¦¨à¦¾à¥¤\",\n",
        "        \"distortion\": \"à¦•à§à¦¯à¦¾à¦Ÿà¦¾à¦¸à¦Ÿà§à¦°à§‹à¦«à¦¾à¦‡à¦œà¦¿à¦‚\",\n",
        "        \"reframed\": \"à¦†à¦®à¦¿ à¦†à¦—à§‡ à¦•à¦·à§à¦Ÿ à¦ªà§‡à¦¯à¦¼à§‡à¦›à¦¿, à¦•à¦¿à¦¨à§à¦¤à§ à¦†à¦®à¦¿ à¦›à§‹à¦Ÿ à¦›à§‹à¦Ÿ à¦§à¦¾à¦ª à¦¨à¦¿à¦¯à¦¼à§‡ à¦à¦Ÿà¦¿ à¦ªà¦°à¦¿à¦šà¦¾à¦²à¦¨à¦¾ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¿à¥¤\"\n",
        "    },\n",
        "    \"anger\": {\n",
        "        \"automatic\": \"à¦¸à§‡ à¦†à¦®à¦¾à¦•à§‡ à¦¸à¦°à§à¦¬à¦¦à¦¾ à¦…à¦¬à¦¹à§‡à¦²à¦¾ à¦•à¦°à§‡ â€” à¦†à¦®à¦¿ à¦¬à¦¿à¦šà¦²à¦¿à¦¤à¥¤\",\n",
        "        \"distortion\": \"à¦œà§‡à¦¨à§‡ à¦¨à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦¬à¦¿à¦¸à§à¦¤à§ƒà¦¤à¦¿\",\n",
        "        \"reframed\": \"à¦¸à¦®à§à¦­à¦¬à¦¤ à¦¤à¦¾à¦°à¦¾ à¦à¦•à¦Ÿà¦¿ à¦­à§à¦² à¦•à¦°à§‡à¦›à§‡ â€” à¦†à¦®à¦¿ à¦¶à¦¾à¦¨à§à¦¤à¦­à¦¾à¦¬à§‡ à¦•à¦¥à¦¾ à¦¬à¦²à§‡à¦‡ à¦¬à¦¿à¦·à¦¯à¦¼à¦Ÿà¦¾ à¦ªà¦°à¦¿à¦·à§à¦•à¦¾à¦° à¦•à¦°à¦¬à¥¤\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def cognitive_reframe(emotion: str, automatic_thought: Optional[str]=None) -> Dict[str,str]:\n",
        "    key = emotion.lower()\n",
        "    example = CBT_EXAMPLES.get(key)\n",
        "    if automatic_thought and key in CBT_EXAMPLES:\n",
        "        # A simple template: label distortion by keywords (very simple rule-based)\n",
        "        distortion = \"à¦•à§‹à¦¯à¦¼à¦¾à¦²à§‡à¦° à¦Ÿà§‡à¦¨à§à¦¡à§‡à¦¨à§à¦¸à¦¿\"\n",
        "        if \"à¦¸à¦¬\" in automatic_thought or \"à¦¸à¦•à¦²\" in automatic_thought:\n",
        "            distortion = \"All-or-nothing thinking\"\n",
        "        return {\n",
        "            \"automatic_thought\": automatic_thought,\n",
        "            \"cognitive_distortion\": distortion,\n",
        "            \"reframed_thought\": CBT_EXAMPLES[key][\"reframed\"]\n",
        "        }\n",
        "    if example:\n",
        "        return {\n",
        "            \"automatic_thought\": example[\"automatic\"],\n",
        "            \"cognitive_distortion\": example[\"distortion\"],\n",
        "            \"reframed_thought\": example[\"reframed\"]\n",
        "        }\n",
        "    # default\n",
        "    return {\n",
        "        \"automatic_thought\": automatic_thought or \"à¦†à¦®à¦¿ à¦à¦®à¦¨à¦Ÿà¦¾à¦‡ à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à¦›à¦¿à¥¤\",\n",
        "        \"cognitive_distortion\": \"à¦¸à¦®à§à¦­à¦¬à¦¤ à¦šà¦¿à¦¨à§à¦¤à¦¾à¦° à¦§à¦°à¦¨ à¦¯à¦¾à¦šà¦¾à¦‡ à¦•à¦°à§à¦¨à¥¤\",\n",
        "        \"reframed_thought\": \"à¦à¦•à¦Ÿà§ à¦¬à¦¿à¦°à¦¤à¦¿ à¦¨à¦¿à¦¨, à¦¤à¦¾à¦°à¦ªà¦° à¦¬à¦¿à¦•à¦²à§à¦ª à¦¸à§à¦ªà¦·à§à¦Ÿà¦­à¦¾à¦¬à§‡ à¦­à¦¾à¦¬à§à¦¨à¥¤\"\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Grounding techniques\n",
        "# ---------------------------\n",
        "GROUNDING = {\n",
        "    \"5_4_3_2_1\": [\n",
        "        \"à§«à¦Ÿà¦¿ à¦œà¦¿à¦¨à¦¿à¦¸ à¦¦à§‡à¦–à§à¦¨ à¦¯à¦¾ à¦†à¦ªà¦¨à¦¿ à¦à¦–à¦¨ à¦¦à§‡à¦–à¦¤à§‡ à¦ªà¦¾à¦šà§à¦›à§‡à¦¨à¥¤\",\n",
        "        \"à§ªà¦Ÿà¦¿ à¦œà¦¿à¦¨à¦¿à¦¸ à¦¸à§à¦ªà¦°à§à¦¶ à¦•à¦°à§à¦¨à¥¤\",\n",
        "        \"à§©à¦Ÿà¦¿ à¦¶à¦¬à§à¦¦ à¦¶à§à¦¨à§à¦¨à¥¤\",\n",
        "        \"à§¨à¦Ÿà¦¿ à¦—à¦¨à§à¦§ à¦šà¦¿à¦¨à§‡ à¦¨à¦¿à¦¨à¥¤\",\n",
        "        \"à§§à¦Ÿà¦¿ à¦¸à§à¦¬à¦¾à¦¦ à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à§à¦¨à¥¤\"\n",
        "    ],\n",
        "    \"breathing\": [\n",
        "        \"à§ª-à§¬-à§® breathing: inhale 4s, hold 6s, exhale 8s â€” repeat 4 à¦¬à¦¾à¦°à¥¤\",\n",
        "        \"à§«-à§«-à§« breathing: inhale 5s, hold 5s, exhale 5s â€” repeat 5 à¦¬à¦¾à¦°à¥¤\"\n",
        "    ],\n",
        "    \"physical\": [\n",
        "        \"à¦ à¦¾à¦¨à§à¦¡à¦¾ à¦œà¦² à¦¦à¦¿à¦¯à¦¼à§‡ à¦®à§à¦– à¦§à§à¦¯à¦¼à§‡ à¦¦à§‡à¦–à¦¾à¥¤\",\n",
        "        \"à¦•à¦ à¦¿à¦¨ à¦•à¦¿à¦›à§ à¦§à¦°à§à¦¨ (stress ball à¦¬à¦¾ à¦¤à¦¾à¦¸) à¦à¦¬à¦‚ à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à§à¦¨à¥¤\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "def grounding_for_emotion(emotion: str) -> Dict[str, List[str]]:\n",
        "    e = emotion.lower()\n",
        "    if e in {\"anxiety\",\"panic\",\"suicidal\"}:\n",
        "        return {\"techniques\": GROUNDING[\"5_4_3_2_1\"] + GROUNDING[\"breathing\"] + GROUNDING[\"physical\"]}\n",
        "    elif e in {\"anger\"}:\n",
        "        return {\"techniques\": [\"à¦¹à¦¾à¦²à¦•à¦¾ à¦¹à¦¾à¦à¦Ÿà¦¾ à§«-à§§à§¦ à¦®à¦¿à¦¨à¦¿à¦Ÿ\", \"à¦—à¦­à§€à¦° à¦¶à§à¦¬à¦¾à¦¸ à¦ªà§à¦°à§à¦¯à¦¾à¦•à¦Ÿà¦¿à¦¸ (à§¬-à§¬-à§¬)\"] + GROUNDING[\"breathing\"]}\n",
        "    elif e in {\"sadness\"}:\n",
        "        return {\"techniques\": [\"à¦•à¦¿à¦›à§ à¦¸à¦®à¦¯à¦¼ à¦¸à§à¦®à§ƒà¦¤à¦¿ à¦²à¦¿à¦–à§‡ à¦«à§‡à¦²à¦¾\", \"à¦à¦•à¦Ÿà¦¿ à¦¸à¦‚à¦¬à§‡à¦¦à¦¨à¦¶à§€à¦² à¦—à¦¾à¦¨ à¦šà¦¾à¦²à¦¾à¦¨à§‹\"] + GROUNDING[\"breathing\"]}\n",
        "    else:\n",
        "        return {\"techniques\": GROUNDING[\"breathing\"]}\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Crisis mode\n",
        "# ---------------------------\n",
        "HOTLINE = {\n",
        "    \"india_general\": \"+91-9152987821\",\n",
        "    \"global_text\": \"If you are in immediate danger, contact local emergency services.\"\n",
        "}\n",
        "\n",
        "def crisis_support(risk_level: str, user_id: Optional[str]=None) -> Dict:\n",
        "    \"\"\"\n",
        "    If risk_level is 'High' or contains 'suicid', return crisis support dict.\n",
        "    \"\"\"\n",
        "    rl = risk_level.lower()\n",
        "    urgent = any(k in rl for k in [\"high\",\"suicid\",\"severe\",\"immediate\"])\n",
        "    msg = {}\n",
        "    if urgent:\n",
        "        msg[\"mode\"] = \"CRISIS_SUPPORT\"\n",
        "        msg[\"message\"] = (\n",
        "            \"à¦†à¦ªà¦¨à¦¿ à¦‰à¦šà§à¦š à¦à§à¦à¦•à¦¿à¦¤à§‡ à¦†à¦›à§‡à¦¨ â€” à¦¦à¦¯à¦¼à¦¾ à¦•à¦°à§‡ à¦¤à¦¾à¦¤à§à¦•à§à¦·à¦£à¦¿à¦•à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦°à¦“ à¦¸à¦¾à¦¥à§‡ à¦¯à§‹à¦—à¦¾à¦¯à§‹à¦— à¦•à¦°à§à¦¨à¥¤ \"\n",
        "            f\"à¦¹à¦Ÿà¦²à¦¾à¦‡à¦¨: {HOTLINE['india_general']}. \"\n",
        "            \"à¦¯à¦¦à¦¿ à¦à¦–à¦¨à¦‡ à¦¬à¦¿à¦ªà¦¦à§‡ à¦¥à¦¾à¦•à§‡à¦¨, à¦¸à§à¦¥à¦¾à¦¨à§€à¦¯à¦¼ à¦œà¦°à§à¦°à¦¿ à¦¸à§‡à¦¬à¦¾à¦¯à¦¼ à¦•à¦² à¦•à¦°à§à¦¨à¥¤\"\n",
        "        )\n",
        "        msg[\"actions\"] = [\n",
        "            \"à¦ à¦¾à¦¨à§à¦¡à¦¾ à¦œà¦² à¦¨à¦¿à¦¨ / à¦®à§à¦– à¦§à§à¦¯à¦¼à§‡ à¦¨à¦¿à¦¨\",\n",
        "            \"à¦¨à¦œà¦°à§‡ à¦†à¦¨à¦¾ à¦•à§‹à¦¨ à¦¬à¦¿à¦¶à§à¦¬à¦¾à¦¸à¦¯à§‹à¦—à§à¦¯ à¦¬à§à¦¯à¦•à§à¦¤à¦¿à¦•à§‡ à¦•à¦² à¦•à¦°à§à¦¨\",\n",
        "            \"à¦¯à¦¦à¦¿ à¦‡à¦¤à¦¿à¦¬à¦¾à¦šà¦• à¦¥à¦¾à¦•à§‡, à¦¹à¦¾à¦¸à¦ªà¦¾à¦¤à¦¾à¦²à§‡ à¦¬à¦¾ à¦œà¦°à§à¦°à¦¿ à¦¸à§‡à¦¬à¦¾à¦¯à¦¼ à¦¯à¦¾à¦¨\"\n",
        "        ]\n",
        "        msg[\"backend_alert\"] = True\n",
        "    else:\n",
        "        msg[\"mode\"] = \"STANDARD\"\n",
        "        msg[\"message\"] = \"à¦†à¦ªà¦¨à¦¾à¦° à¦°à¦¿à¦¸à§à¦• à¦ªà¦°à§à¦¯à¦¾à¦¯à¦¼ à¦ªà¦°à§à¦¯à¦¬à§‡à¦•à§à¦·à¦£ à¦•à¦°à¦¾ à¦¹à¦šà§à¦›à§‡ â€” à¦•à¦¯à¦¼à§‡à¦•à¦Ÿà¦¿ à¦¸à¦¹à¦¾à¦¯à¦¼à¦• à¦Ÿà¦¿à¦ªà¦¸ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¹à¦¬à§‡à¥¤\"\n",
        "        msg[\"backend_alert\"] = False\n",
        "    if user_id:\n",
        "        msg[\"personalized\"] = f\"{user_id}, {msg['message']}\"\n",
        "    return msg\n",
        "\n",
        "# ---------------------------\n",
        "# 8) Empathy Layer\n",
        "# ---------------------------\n",
        "def empathy_response(emotion: str, intensity: Optional[int]=None) -> str:\n",
        "    e = emotion.lower()\n",
        "    if e == \"sadness\" or e == \"sad\":\n",
        "        base = \"à¦†à¦®à¦¿ à¦¦à§à¦ƒà¦–à¦¿à¦¤ à¦†à¦ªà¦¨à¦¿ à¦à¦®à¦¨à¦Ÿà¦¿ à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à¦›à§‡à¦¨à¥¤ à¦à¦Ÿà¦¾ à¦¸à§à¦¬à¦¾à¦­à¦¾à¦¬à¦¿à¦• à¦à¦¬à¦‚ à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦¾ à¦¨à¦¨à¥¤\"\n",
        "    elif e == \"anxiety\":\n",
        "        base = \"à¦†à¦®à¦¿ à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à¦›à¦¿ à¦†à¦ªà¦¨à¦¿ à¦‰à¦¦à§à¦¬à¦¿à¦—à§à¦¨â€”à¦à¦‡ à¦…à¦¨à§à¦­à§‚à¦¤à¦¿ à¦•à¦ à¦¿à¦¨ à¦•à¦¿à¦¨à§à¦¤à§ à¦…à¦¸à§à¦¥à¦¾à§Ÿà§€à¥¤\"\n",
        "    elif e == \"anger\":\n",
        "        base = \"à¦†à¦ªà¦¨à¦¿ à¦°à§‡à¦—à§‡ à¦†à¦›à§‡à¦¨â€”à¦à¦Ÿà¦¾ à¦¸à§à¦¬à¦¾à¦­à¦¾à¦¬à¦¿à¦• à¦“ à¦•à¦°à§à¦¤à¦¬à§à¦¯à¦ªà§‚à¦°à§à¦£à¥¤ à¦§à§€à¦°à§‡ à¦•à¦¥à¦¾ à¦¬à¦²à¦¾à¦Ÿà¦¾ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯ à¦•à¦°à¦¬à§‡à¥¤\"\n",
        "    elif e == \"joy\" or e == \"happy\":\n",
        "        base = \"à¦–à§à¦¬ à¦­à¦¾à¦²à§‹ à¦²à¦¾à¦—à¦² à¦à¦‡ à¦–à¦¬à¦° à¦¶à§à¦¨à§‡! à¦†à¦ªà¦¨à¦¾à¦° à¦†à¦¨à¦¨à§à¦¦ à¦¶à§‡à¦¯à¦¼à¦¾à¦° à¦•à¦°à¦¾ à¦šà¦¾à¦²à¦¿à¦¯à¦¼à§‡ à¦¯à¦¾à¦¨à¥¤\"\n",
        "    elif e == \"suicidal\":\n",
        "        base = \"à¦†à¦®à¦¿ à¦‰à¦¦à§à¦¬à¦¿à¦—à§à¦¨ â€” à¦†à¦ªà¦¨à¦¿ à¦¨à¦¿à¦°à¦¾à¦ªà¦¦ à¦¨à¦¾ à¦¹à¦²à§‡ à¦¦à¦¯à¦¼à¦¾ à¦•à¦°à§‡ à¦…à¦¬à¦¿à¦²à¦®à§à¦¬à§‡ à¦•à¦¾à¦°à§‹ à¦¸à¦¾à¦¥à§‡ à¦¯à§‹à¦—à¦¾à¦¯à§‹à¦— à¦•à¦°à§à¦¨à¥¤\"\n",
        "    else:\n",
        "        base = \"à¦†à¦®à¦¿ à¦¶à§à¦¨à¦¤à§‡ à¦ªà§‡à¦¯à¦¼à§‡à¦›à¦¿â€”à¦†à¦ªà¦¨à¦¿ à¦¯à¦¾ à¦…à¦¨à§à¦­à¦¬ à¦•à¦°à¦›à§‡à¦¨ à¦¤à¦¾ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£à¥¤\"\n",
        "    if intensity:\n",
        "        base += f\" (Intensity: {intensity}/10)\"\n",
        "    return base\n",
        "\n",
        "# ---------------------------\n",
        "# 9) Wrapper: generate_full_support\n",
        "# ---------------------------\n",
        "def generate_full_support(\n",
        "    user_id: str,\n",
        "    emotion: str,\n",
        "    intensity: int,\n",
        "    history: List[Dict],\n",
        "    automatic_thought: Optional[str]=None,\n",
        "    risk_level: Optional[str]=None\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Generates full support package:\n",
        "    - Append history\n",
        "    - Follow-up question\n",
        "    - Insights (last 7 days)\n",
        "    - Music recs\n",
        "    - CBT reframing\n",
        "    - Grounding tips\n",
        "    - Crisis support if needed\n",
        "    - Empathy sentence\n",
        "    \"\"\"\n",
        "    append_mood(history, emotion, intensity)\n",
        "\n",
        "    followup = generate_followup_question(emotion)\n",
        "    insights = generate_weekly_insights(history)\n",
        "    music = recommend_music(emotion)\n",
        "    cbt = cognitive_reframe(emotion, automatic_thought)\n",
        "    grounding = grounding_for_emotion(emotion)\n",
        "    empathy = empathy_response(emotion, intensity)\n",
        "    crisis = crisis_support(risk_level or (\"low\" if intensity < 6 else \"medium\"), user_id=user_id)\n",
        "\n",
        "    support = {\n",
        "        \"user_id\": user_id,\n",
        "        \"emotion\": emotion,\n",
        "        \"intensity\": intensity,\n",
        "        \"followup_question\": followup,\n",
        "        \"insights\": insights,\n",
        "        \"music_recommendations\": music,\n",
        "        \"cognitive_reframe\": cbt,\n",
        "        \"grounding\": grounding,\n",
        "        \"empathy_message\": empathy,\n",
        "        \"crisis\": crisis\n",
        "    }\n",
        "    return support\n",
        "\n",
        "# ---------------------------\n",
        "# Example usage\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # sample history\n",
        "    history = []\n",
        "    # append some demo events\n",
        "    append_mood(history, \"joy\", 6, datetime.datetime.now() - datetime.timedelta(days=3))\n",
        "    append_mood(history, \"anxiety\", 7, datetime.datetime.now() - datetime.timedelta(days=2, hours=4))\n",
        "    append_mood(history, \"sadness\", 5, datetime.datetime.now() - datetime.timedelta(days=1))\n",
        "    # current event\n",
        "    user_id = \"user\"\n",
        "    emotion = f\n",
        "    intensity = 8\n",
        "    automatic_thought = \"à¦†à¦®à¦¿ à¦¸à¦¬ à¦•à¦¿à¦›à§à¦‡ à¦­à§‡à¦™à§‡ à¦«à§‡à¦²à¦¬à§‹\"\n",
        "    risk_level = \"High emotional risk\"\n",
        "\n",
        "    result = generate_full_support(user_id, emotion, intensity, history, automatic_thought, risk_level)\n",
        "    import json\n",
        "    print(json.dumps(result, ensure_ascii=False, indent=2))\n",
        "\n",
        "    # Plot recent mood trend\n",
        "    plot_mood_trend(history + [{\"timestamp\": datetime.datetime.now(), \"emotion\": emotion, \"intensity\": intensity}])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oPdluV_0pn0c"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.clear()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}